{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Transformer Demo\n",
    "\n",
    "This notebook demonstrates the Trajectory Transformer (TT) model, which models full trajectories autoregressively:\n",
    "\n",
    "**Key Features:**\n",
    "- Models sequence: s₀, a₀, r₀, s₁, a₁, r₁, ...\n",
    "- Single transformer predicts all tokens (states, actions, rewards)\n",
    "- Multi-head outputs for different prediction tasks\n",
    "- Can be used for both behavior cloning and planning\n",
    "\n",
    "**Sections:**\n",
    "1. Generate synthetic trajectory data\n",
    "2. Train Trajectory Transformer\n",
    "3. Evaluate world model accuracy\n",
    "4. Beam search planning\n",
    "5. Comparison with other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Autoregressive Trajectory Modeling** - Predicting full sequences (s₀, a₀, r₀, s₁, a₁, r₁, ...)\n",
    "2. **World Models** - Learning environment dynamics to predict future states and rewards\n",
    "3. **Beam Search Planning** - Looking ahead multiple steps to select better actions\n",
    "4. **Multi-Task Learning** - Joint training on state, action, and reward prediction\n",
    "5. **Planning vs Reactive Policies** - When lookahead improves performance\n",
    "\n",
    "**Estimated Time**: 30-40 minutes (includes data collection and training)\n",
    "**Prerequisites**: Understanding of transformers, sequence modeling, planning concepts\n",
    "**Hardware**: GPU recommended for transformer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from models import TrajectoryTransformer, TrajectoryTransformerConfig, create_trajectory_transformer\n",
    "from training import TrajectoryTransformerTrainer, TrainingConfig, create_trainer\n",
    "from training import BeamSearchPlanner, PlannerConfig, create_planner\n",
    "from poc.atc_rl import Simple2DATCEnv\n",
    "from environment import get_device\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = get_device()  # Auto-detects CUDA, Metal (MPS), or CPU\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
    "elif device == \"mps\":\n",
    "    print(\"Metal Performance Shaders (Apple Silicon GPU) enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Trajectory Data\n",
    "\n",
    "We'll use the Simple2DATCEnv to generate trajectory data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_observation(obs):\n",
    "    \"\"\"Flatten observation dict to single vector.\"\"\"\n",
    "    aircraft_flat = obs[\"aircraft\"].flatten()  # (max_aircraft * 6,)\n",
    "    global_state = obs[\"global_state\"]  # (4,)\n",
    "    return np.concatenate([aircraft_flat, global_state])\n",
    "\n",
    "def action_to_index(action):\n",
    "    \"\"\"Convert multi-discrete action to single index.\"\"\"\n",
    "    # Action space: [21, 5, 18, 13, 8]\n",
    "    # Total: 21 * 5 * 18 * 13 * 8 = 196,560 actions (too large)\n",
    "    # Simplification: use just aircraft_id * 5 + command_type\n",
    "    aircraft_id, command_type, altitude, heading, speed = action\n",
    "    return aircraft_id * 5 + command_type\n",
    "\n",
    "def collect_episodes(env, num_episodes=1000, max_steps=50):\n",
    "    \"\"\"Collect trajectory data from environment.\"\"\"\n",
    "    episodes = []\n",
    "    \n",
    "    for ep in tqdm(range(num_episodes), desc=\"Collecting episodes\"):\n",
    "        obs, info = env.reset()\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Flatten observation\n",
    "            state = flatten_observation(obs)\n",
    "            states.append(state)\n",
    "            \n",
    "            # Random action\n",
    "            action = env.action_space.sample()\n",
    "            action_idx = action_to_index(action)\n",
    "            actions.append(action_idx)\n",
    "            \n",
    "            # Step environment\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        episodes.append({\n",
    "            \"states\": np.array(states),\n",
    "            \"actions\": np.array(actions),\n",
    "            \"rewards\": np.array(rewards),\n",
    "        })\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Create environment\n",
    "env = Simple2DATCEnv(max_aircraft=5, max_steps=50)\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "# Get dimensions\n",
    "obs, _ = env.reset()\n",
    "state_dim = flatten_observation(obs).shape[0]\n",
    "action_dim = 21 * 5  # Simplified action space\n",
    "\n",
    "print(f\"\\nState dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect training data\n",
    "# ⏱️ ~5-10 minutes for 500 episodes\n",
    "print(\"Collecting training episodes...\")\n",
    "episodes = collect_episodes(env, num_episodes=500, max_steps=50)\n",
    "\n",
    "print(f\"\\nCollected {len(episodes)} episodes\")\n",
    "print(f\"Average episode length: {np.mean([len(ep['states']) for ep in episodes]):.1f}\")\n",
    "print(f\"Average episode return: {np.mean([ep['rewards'].sum() for ep in episodes]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad episodes to same length\n",
    "max_steps = 50\n",
    "\n",
    "states_array = np.zeros((len(episodes), max_steps, state_dim), dtype=np.float32)\n",
    "actions_array = np.zeros((len(episodes), max_steps), dtype=np.int64)\n",
    "rewards_array = np.zeros((len(episodes), max_steps), dtype=np.float32)\n",
    "\n",
    "for i, ep in enumerate(episodes):\n",
    "    ep_len = len(ep[\"states\"])\n",
    "    states_array[i, :ep_len] = ep[\"states\"]\n",
    "    actions_array[i, :ep_len] = ep[\"actions\"]\n",
    "    rewards_array[i, :ep_len] = ep[\"rewards\"]\n",
    "\n",
    "print(f\"States shape: {states_array.shape}\")\n",
    "print(f\"Actions shape: {actions_array.shape}\")\n",
    "print(f\"Rewards shape: {rewards_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Trajectory Transformer\n",
    "\n",
    "Train the model to predict states, actions, and rewards autoregressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = create_trainer(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    embed_dim=128,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    ff_dim=512,\n",
    "    context_length=20,\n",
    "    batch_size=32,\n",
    "    num_epochs=50,\n",
    "    learning_rate=3e-4,\n",
    "    state_loss_weight=1.0,\n",
    "    action_loss_weight=1.0,\n",
    "    reward_loss_weight=0.5,\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in trainer.model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# ⏱️ ~10-15 minutes on GPU\n",
    "print(\"Training Trajectory Transformer...\")\n",
    "trainer.train(states_array, actions_array, rewards_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "metrics = trainer.get_metrics()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(metrics[\"train_losses\"])\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Evaluation losses\n",
    "if metrics[\"eval_losses\"]:\n",
    "    eval_steps = np.arange(len(metrics[\"eval_losses\"]))\n",
    "    total_losses = [l[\"total_loss\"] for l in metrics[\"eval_losses\"]]\n",
    "    state_losses = [l[\"state_loss\"] for l in metrics[\"eval_losses\"]]\n",
    "    action_losses = [l[\"action_loss\"] for l in metrics[\"eval_losses\"]]\n",
    "    reward_losses = [l[\"reward_loss\"] for l in metrics[\"eval_losses\"]]\n",
    "    \n",
    "    axes[1].plot(eval_steps, total_losses, label=\"Total\", linewidth=2)\n",
    "    axes[1].plot(eval_steps, state_losses, label=\"State\", alpha=0.7)\n",
    "    axes[1].plot(eval_steps, action_losses, label=\"Action\", alpha=0.7)\n",
    "    axes[1].plot(eval_steps, reward_losses, label=\"Reward\", alpha=0.7)\n",
    "    axes[1].set_xlabel(\"Evaluation Step\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].set_title(\"Validation Loss Components\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation loss: {metrics['best_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate World Model Accuracy\n",
    "\n",
    "Test how well the model predicts next states, actions, and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create planner for evaluation\n",
    "# ⏱️ ~1-2 seconds per episode\n",
    "planner = create_planner(\n",
    "    trainer.model,\n",
    "    beam_width=5,\n",
    "    lookahead_steps=5,\n",
    ")\n",
    "\n",
    "# Evaluate on a few test episodes\n",
    "num_test = 10\n",
    "test_indices = np.random.choice(len(states_array), num_test, replace=False)\n",
    "\n",
    "eval_results = []\n",
    "for idx in test_indices:\n",
    "    states = torch.FloatTensor(states_array[idx:idx+1])\n",
    "    actions = torch.LongTensor(actions_array[idx:idx+1])\n",
    "    rewards = torch.FloatTensor(rewards_array[idx:idx+1]).unsqueeze(-1)\n",
    "    \n",
    "    result = planner.evaluate_trajectory(states, actions, rewards)\n",
    "    eval_results.append(result)\n",
    "\n",
    "# Print average metrics\n",
    "print(\"World Model Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "for key in eval_results[0].keys():\n",
    "    avg_value = np.mean([r[key] for r in eval_results])\n",
    "    print(f\"{key:25s}: {avg_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for one episode\n",
    "test_idx = test_indices[0]\n",
    "states_test = torch.FloatTensor(states_array[test_idx:test_idx+1])\n",
    "actions_test = torch.LongTensor(actions_array[test_idx:test_idx+1])\n",
    "rewards_test = torch.FloatTensor(rewards_array[test_idx:test_idx+1]).unsqueeze(-1)\n",
    "\n",
    "# Get predictions\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_states, pred_actions, pred_rewards = trainer.model(\n",
    "        states_test.to(trainer.config.device),\n",
    "        actions_test.to(trainer.config.device),\n",
    "        rewards_test.to(trainer.config.device)\n",
    "    )\n",
    "\n",
    "# Convert to numpy\n",
    "pred_rewards_np = pred_rewards.cpu().numpy()[0, :, 0]\n",
    "actual_rewards_np = rewards_test.numpy()[0, :, 0]\n",
    "pred_actions_np = pred_actions.argmax(dim=-1).cpu().numpy()[0]\n",
    "actual_actions_np = actions_test.numpy()[0]\n",
    "\n",
    "# Find valid timesteps (non-zero rewards or actions)\n",
    "valid_mask = (actual_rewards_np != 0) | (actual_actions_np != 0)\n",
    "valid_steps = np.where(valid_mask)[0]\n",
    "if len(valid_steps) > 0:\n",
    "    max_step = valid_steps[-1] + 1\n",
    "else:\n",
    "    max_step = 20\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Reward prediction\n",
    "axes[0].plot(actual_rewards_np[:max_step], label=\"Actual\", marker='o', alpha=0.7)\n",
    "axes[0].plot(pred_rewards_np[:max_step], label=\"Predicted\", marker='x', alpha=0.7)\n",
    "axes[0].set_xlabel(\"Timestep\")\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "axes[0].set_title(\"Reward Prediction\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Action prediction\n",
    "axes[1].plot(actual_actions_np[:max_step], label=\"Actual\", marker='o', alpha=0.7)\n",
    "axes[1].plot(pred_actions_np[:max_step], label=\"Predicted\", marker='x', alpha=0.7)\n",
    "axes[1].set_xlabel(\"Timestep\")\n",
    "axes[1].set_ylabel(\"Action Index\")\n",
    "axes[1].set_title(\"Action Prediction\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute accuracy for this episode\n",
    "action_matches = (pred_actions_np[:max_step] == actual_actions_np[:max_step])\n",
    "action_accuracy = action_matches.sum() / max_step\n",
    "reward_mse = np.mean((pred_rewards_np[:max_step] - actual_rewards_np[:max_step]) ** 2)\n",
    "\n",
    "print(f\"Episode {test_idx}:\")\n",
    "print(f\"  Action accuracy: {action_accuracy:.2%}\")\n",
    "print(f\"  Reward MSE: {reward_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Beam Search Planning\n",
    "\n",
    "Use the trained world model for planning with beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test beam search planning\n",
    "# ⏱️ ~20-30 seconds for 10 episodes\n",
    "def test_planning(env, planner, num_episodes=5, max_steps=50):\n",
    "    \"\"\"Test planning in environment.\"\"\"\n",
    "    episode_returns = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        \n",
    "        states_history = []\n",
    "        actions_history = []\n",
    "        rewards_history = []\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Flatten observation\n",
    "            state = flatten_observation(obs)\n",
    "            states_history.append(state)\n",
    "            \n",
    "            # Convert history to tensors\n",
    "            if len(states_history) > 1:\n",
    "                states_tensor = torch.FloatTensor(np.array(states_history)).unsqueeze(0)\n",
    "                actions_tensor = torch.LongTensor(np.array(actions_history)).unsqueeze(0)\n",
    "                rewards_tensor = torch.FloatTensor(np.array(rewards_history)).unsqueeze(0).unsqueeze(-1)\n",
    "            else:\n",
    "                states_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)\n",
    "                actions_tensor = None\n",
    "                rewards_tensor = None\n",
    "            \n",
    "            # Plan action\n",
    "            action_idx = planner.plan(states_tensor, actions_tensor, rewards_tensor).item()\n",
    "            \n",
    "            # Convert to environment action\n",
    "            aircraft_id = action_idx // 5\n",
    "            command_type = action_idx % 5\n",
    "            action = np.array([aircraft_id, command_type, 0, 0, 0])\n",
    "            \n",
    "            actions_history.append(action_idx)\n",
    "            \n",
    "            # Step environment\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        episode_returns.append(total_reward)\n",
    "        print(f\"Episode {ep + 1}: Return = {total_reward:.2f}, Steps = {len(states_history)}\")\n",
    "    \n",
    "    return episode_returns\n",
    "\n",
    "print(\"Testing beam search planning...\")\n",
    "planning_returns = test_planning(env, planner, num_episodes=10)\n",
    "\n",
    "print(f\"\\nPlanning Results:\")\n",
    "print(f\"  Average return: {np.mean(planning_returns):.2f} +/- {np.std(planning_returns):.2f}\")\n",
    "print(f\"  Max return: {np.max(planning_returns):.2f}\")\n",
    "print(f\"  Min return: {np.min(planning_returns):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Planning Strategies\n",
    "\n",
    "Compare beam search planning with random actions and greedy action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⏱️ ~2-3 minutes total (3 strategies × 20 episodes)\n",
    "def test_random_policy(env, num_episodes=10, max_steps=50):\n",
    "    \"\"\"Test random policy.\"\"\"\n",
    "    episode_returns = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        episode_returns.append(total_reward)\n",
    "    \n",
    "    return episode_returns\n",
    "\n",
    "def test_greedy_policy(env, model, num_episodes=10, max_steps=50):\n",
    "    \"\"\"Test greedy action selection (no planning).\"\"\"\n",
    "    episode_returns = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        \n",
    "        states_history = []\n",
    "        actions_history = []\n",
    "        rewards_history = []\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            state = flatten_observation(obs)\n",
    "            states_history.append(state)\n",
    "            \n",
    "            # Get action from model (no planning, just immediate prediction)\n",
    "            if len(states_history) > 1:\n",
    "                states_tensor = torch.FloatTensor(np.array(states_history)).unsqueeze(0)\n",
    "                actions_tensor = torch.LongTensor(np.array(actions_history)).unsqueeze(0)\n",
    "                rewards_tensor = torch.FloatTensor(np.array(rewards_history)).unsqueeze(0).unsqueeze(-1)\n",
    "            else:\n",
    "                states_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)\n",
    "                actions_tensor = None\n",
    "                rewards_tensor = None\n",
    "            \n",
    "            action_idx = model.get_action(states_tensor, actions_tensor, rewards_tensor, sample=False).item()\n",
    "            \n",
    "            aircraft_id = action_idx // 5\n",
    "            command_type = action_idx % 5\n",
    "            action = np.array([aircraft_id, command_type, 0, 0, 0])\n",
    "            \n",
    "            actions_history.append(action_idx)\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        episode_returns.append(total_reward)\n",
    "    \n",
    "    return episode_returns\n",
    "\n",
    "# Compare strategies\n",
    "print(\"Testing different strategies...\\n\")\n",
    "\n",
    "print(\"1. Random Policy:\")\n",
    "random_returns = test_random_policy(env, num_episodes=20)\n",
    "print(f\"   Mean: {np.mean(random_returns):.2f} +/- {np.std(random_returns):.2f}\\n\")\n",
    "\n",
    "print(\"2. Greedy Policy (no planning):\")\n",
    "greedy_returns = test_greedy_policy(env, trainer.model, num_episodes=20)\n",
    "print(f\"   Mean: {np.mean(greedy_returns):.2f} +/- {np.std(greedy_returns):.2f}\\n\")\n",
    "\n",
    "print(\"3. Beam Search Planning:\")\n",
    "planning_returns_full = test_planning(env, planner, num_episodes=20)\n",
    "print(f\"   Mean: {np.mean(planning_returns_full):.2f} +/- {np.std(planning_returns_full):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "strategies = ['Random', 'Greedy', 'Beam Search']\n",
    "returns = [random_returns, greedy_returns, planning_returns_full]\n",
    "colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\n",
    "\n",
    "bp = ax.boxplot(returns, labels=strategies, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Episode Return')\n",
    "ax.set_title('Comparison of Action Selection Strategies')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nDetailed Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "for strategy, returns_list in zip(strategies, returns):\n",
    "    print(f\"{strategy:15s}: {np.mean(returns_list):6.2f} +/- {np.std(returns_list):5.2f} \"\n",
    "          f\"(min: {np.min(returns_list):6.2f}, max: {np.max(returns_list):6.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls & Troubleshooting\n",
    "\n",
    "### Problem 1: \"World model predictions diverge quickly\"\n",
    "**Solution**: Error accumulates during autoregressive prediction. Mitigate with:\n",
    "- **Shorter planning horizons**: Reduce lookahead from 10 to 3-5 steps\n",
    "- **More training data**: Collect 2000+ episodes\n",
    "- **Larger model**: Increase layers from 4 to 6\n",
    "\n",
    "```python\n",
    "planner = create_planner(\n",
    "    model,\n",
    "    lookahead_steps=3,  # Shorter horizon\n",
    ")\n",
    "```\n",
    "\n",
    "### Problem 2: Beam search worse than greedy selection\n",
    "**Causes**:\n",
    "- **World model inaccurate**: Predictions unreliable for planning\n",
    "- **Beam width too large**: Exploring low-probability branches\n",
    "- **Reward model poor**: Incorrect reward predictions guide planning wrong\n",
    "\n",
    "**Solution**: Train world model longer or use smaller beam width:\n",
    "```python\n",
    "config = PlannerConfig(\n",
    "    beam_width=3,  # Reduce from 5\n",
    "    lookahead_steps=3,  # Shorter horizon\n",
    ")\n",
    "```\n",
    "\n",
    "### Problem 3: \"Training is very slow (>1 hour for 50 epochs)\"\n",
    "**Solution**: \n",
    "- Use GPU acceleration\n",
    "- Reduce sequence length\n",
    "- Smaller model (fewer layers/heads)\n",
    "\n",
    "```python\n",
    "config = TrainingConfig(\n",
    "    num_layers=3,  # Down from 4\n",
    "    num_heads=4,   # Down from 8\n",
    ")\n",
    "```\n",
    "\n",
    "### Problem 4: Action prediction accuracy low (<60%)\n",
    "**Causes**:\n",
    "- **Simplified action space too large**: Reduce action space\n",
    "- **Not enough training data**: Collect more episodes\n",
    "- **Model underfitting**: Increase model capacity\n",
    "\n",
    "**Solution**: Simplify action space further or collect better data:\n",
    "```python\n",
    "def action_to_index(action):\n",
    "    # Use only aircraft_id and command_type (ignore parameters)\n",
    "    return action[0] * 5 + action[1]\n",
    "```\n",
    "\n",
    "### Problem 5: State predictions look wrong visually\n",
    "**This is expected!** States are high-dimensional and hard to predict exactly. Focus on:\n",
    "- Reward prediction accuracy (more important for planning)\n",
    "- Action prediction accuracy (indicates model understands task)\n",
    "- Beam search performance (ultimate metric)\n",
    "\n",
    "### Problem 6: \"RuntimeError: sequence length mismatch\"\n",
    "**Solution**: Ensure consistent padding across states, actions, rewards:\n",
    "```python\n",
    "# All should have same length (max_steps)\n",
    "states_array = np.zeros((num_episodes, max_steps, state_dim))\n",
    "actions_array = np.zeros((num_episodes, max_steps))\n",
    "rewards_array = np.zeros((num_episodes, max_steps))\n",
    "```\n",
    "\n",
    "### Debugging Tips:\n",
    "1. **Visualize one-step predictions**: Check accuracy before multi-step\n",
    "2. **Compare beam widths**: Plot performance vs beam_width (1, 3, 5, 10)\n",
    "3. **Analyze planning failures**: When does beam search pick wrong actions?\n",
    "4. **Test on simple scenarios**: Start with 2-3 aircraft before scaling\n",
    "\n",
    "**Need more help?** See Trajectory Transformer paper or model-based RL literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Trajectory Transformer Key Insights:**\n",
    "\n",
    "1. **Unified Model**: Single transformer predicts states, actions, and rewards\n",
    "2. **World Model**: Can predict future trajectories with reasonable accuracy\n",
    "3. **Planning**: Beam search enables looking ahead and selecting better actions\n",
    "4. **Multi-task Learning**: Joint training on all three prediction tasks helps regularization\n",
    "\n",
    "**Comparison with Other Approaches:**\n",
    "\n",
    "- **vs. Decision Transformer**: TT models the full trajectory including states and rewards, enabling planning\n",
    "- **vs. PPO**: TT is offline and can be trained on any trajectory data, but may need more data\n",
    "- **vs. Random**: TT learns patterns from data, significantly outperforming random actions\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. Train on higher quality data (from expert demonstrations or trained policies)\n",
    "2. Experiment with different beam widths and lookahead depths\n",
    "3. Fine-tune on specific scenarios or objectives\n",
    "4. Use the world model for model-based RL or data augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
