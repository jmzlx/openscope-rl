{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conservative Q-Learning (CQL) for Offline RL\n",
        "\n",
        "This notebook demonstrates Conservative Q-Learning (CQL), a value-based offline RL method that learns conservative Q-values to avoid distribution shift when training from fixed datasets.\n",
        "\n",
        "## Why CQL?\n",
        "- Works well with mixed-quality offline datasets\n",
        "- No transformers or sequence modeling required\n",
        "- Simple value-based learning with a conservative penalty\n",
        "\n",
        "## Workflow\n",
        "1. Load offline dataset of (s, a, r, s', done)\n",
        "2. Train Q-network with CQL loss\n",
        "3. Evaluate learned Q-function on held-out data\n",
        "4. (Optional) Derive a policy from the Q-function and evaluate in OpenScope\n",
        "\n",
        "## Prerequisites\n",
        "- Offline dataset collected from OpenScope (random/heuristic policies)\n",
        "- GPU recommended for faster training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Imports\n",
        "\n",
        "Let's import the necessary modules and set up the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from data.offline_dataset import OfflineDatasetCollector, Episode\n",
        "from training.cql_trainer import CQLTrainer, CQLConfig\n",
        "from environment.utils import get_device\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Load or Collect Offline Dataset\n",
        "\n",
        "You can either:\n",
        "- Load a pre-collected dataset from disk, or\n",
        "- Collect a small dataset now (slower; requires OpenScope server)\n",
        "\n",
        "We'll show both patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Load pre-collected dataset\n",
        "from pathlib import Path\n",
        "from data.offline_dataset import OfflineDatasetCollector\n",
        "\n",
        "data_path = Path(\"../data/offline_data.pkl\")\n",
        "\n",
        "if data_path.exists():\n",
        "    print(f\"ðŸ“¦ Loading episodes from {data_path}\")\n",
        "    episodes = OfflineDatasetCollector.load_episodes(str(data_path))\n",
        "else:\n",
        "    print(\"âš ï¸ No dataset found at ../data/offline_data.pkl. You can collect a small one now.\")\n",
        "    episodes = []\n",
        "\n",
        "print(f\"Episodes loaded: {len(episodes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option B: Collect a small dataset now (requires OpenScope server)\n",
        "from environment import PlaywrightEnv, create_default_config\n",
        "\n",
        "if len(episodes) == 0:\n",
        "    try:\n",
        "        print(\"ðŸŽ¬ Collecting a small dataset (random policy)...\")\n",
        "        env = PlaywrightEnv(headless=True, timewarp=5, max_aircraft=5, episode_length=300)\n",
        "        collector = OfflineDatasetCollector(env)\n",
        "        episodes = collector.collect_random_episodes(num_episodes=25, max_steps=100, verbose=True)\n",
        "        print(f\"Collected {len(episodes)} episodes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to collect data: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Train CQL\n",
        "\n",
        "Let's configure and train the Conservative Q-Learning trainer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(episodes) == 0:\n",
        "    raise RuntimeError(\"No offline episodes available. Please load or collect data first.\")\n",
        "\n",
        "# Configure CQL\n",
        "config = CQLConfig(\n",
        "    max_aircraft=5,\n",
        "    num_epochs=10,       # keep small for demo\n",
        "    batch_size=256,\n",
        "    learning_rate=3e-4,\n",
        "    cql_alpha=5.0,\n",
        ")\n",
        "\n",
        "trainer = CQLTrainer(config)\n",
        "history = trainer.train(episodes)\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "print({k: v[-1] for k, v in history.items() if len(v) > 0})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Visualize Training Metrics\n",
        "\n",
        "Plot CQL losses over epochs to verify training behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].plot(history.get(\"train_losses\", []))\n",
        "axes[0].set_title(\"Total Loss\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(history.get(\"td_losses\", []))\n",
        "axes[1].set_title(\"TD Loss\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Loss\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "axes[2].plot(history.get(\"cql_losses\", []))\n",
        "axes[2].set_title(\"CQL Penalty\")\n",
        "axes[2].set_xlabel(\"Epoch\")\n",
        "axes[2].set_ylabel(\"Loss\")\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Greedy Policy Extraction (Optional)\n",
        "\n",
        "Extract a simple policy by sampling candidate actions and choosing the one with highest Q(s, a). This is approximate and for demonstration only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_greedy_action(q_network, obs, num_candidates: int = 64):\n",
        "    \"\"\"Sample num_candidates actions and return the highest-Q one.\"\"\"\n",
        "    device = next(q_network.parameters()).device\n",
        "    # Convert single obs dict to tensors with batch dim 1\n",
        "    obs_t = {\n",
        "        'aircraft': torch.from_numpy(obs['aircraft']).float().unsqueeze(0).to(device),\n",
        "        'aircraft_mask': torch.from_numpy(obs['aircraft_mask']).bool().unsqueeze(0).to(device),\n",
        "        'global_state': torch.from_numpy(obs['global_state']).float().unsqueeze(0).to(device),\n",
        "    }\n",
        "\n",
        "    best_q = None\n",
        "    best_action = None\n",
        "\n",
        "    for _ in range(num_candidates):\n",
        "        # Sample random action\n",
        "        act = {\n",
        "            'aircraft_id': torch.randint(0, 6, (1,), device=device),\n",
        "            'command_type': torch.randint(0, 5, (1,), device=device),\n",
        "            'altitude': torch.randint(0, 18, (1,), device=device),\n",
        "            'heading': torch.randint(0, 13, (1,), device=device),\n",
        "            'speed': torch.randint(0, 8, (1,), device=device),\n",
        "        }\n",
        "        q = q_network(obs_t, act).squeeze()\n",
        "        if best_q is None or q.item() > best_q:\n",
        "            best_q = q.item()\n",
        "            best_action = {k: v.item() for k, v in act.items()}\n",
        "\n",
        "    return best_action, best_q\n",
        "\n",
        "# Example usage (requires a fresh observation 'obs'):\n",
        "# obs, _ = env.reset()\n",
        "# action, q_value = sample_greedy_action(trainer.q_network, obs)\n",
        "# print(action, q_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "- Trained a Conservative Q-Learning (CQL) Q-network from offline OpenScope trajectories.\n",
        "- Visualized TD and conservative losses to verify stable training.\n",
        "- Provided a simple greedy action extractor for demo purposes.\n",
        "\n",
        "Next:\n",
        "- Evaluate greedy policy in OpenScope (short rollout).\n",
        "- Compare CQL vs Decision Transformer on sample efficiency and stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conservative Q-Learning (CQL) for Offline RL\n",
        "\n",
        "This notebook demonstrates **Conservative Q-Learning (CQL)**, a value-based offline RL approach that learns Q-functions conservatively to prevent distribution shift.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "**Traditional Q-Learning:**\n",
        "- Learns Q(s, a) via TD learning\n",
        "- Can overestimate Q-values for out-of-distribution actions\n",
        "- Distribution shift when deploying offline-trained policies\n",
        "\n",
        "**Conservative Q-Learning (CQL):**\n",
        "- Adds conservative penalty to Q-learning\n",
        "- Minimizes Q-values for actions NOT in dataset\n",
        "- Prevents overestimation of OOD actions\n",
        "- More robust offline RL than standard Q-learning\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Collect offline data** - Random and heuristic policies\n",
        "2. **Train CQL Q-network** - Learn conservative Q-function\n",
        "3. **Extract policy** - Greedy policy from learned Q-function\n",
        "4. **Evaluate** - Test policy on environment\n",
        "5. **Compare** - CQL vs Decision Transformer vs PPO\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- OpenScope server running at http://localhost:3003 (for data collection)\n",
        "- GPU recommended for faster training\n",
        "- Estimated time: 40-50 minutes (data collection + training)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **Value-Based Offline RL** - Learning Q-functions from fixed datasets\n",
        "2. **Conservative Regularization** - Preventing distribution shift via conservative updates\n",
        "3. **CQL Algorithm** - How conservative penalties work in Q-learning\n",
        "4. **Policy Extraction** - Getting policies from Q-functions (greedy action selection)\n",
        "5. **CQL vs DT** - When to use value-based vs sequence modeling approaches\n",
        "\n",
        "**Estimated Time**: 40-50 minutes (includes data collection and training)  \n",
        "**Prerequisites**: Understanding of Q-learning, basic RL concepts  \n",
        "**Hardware**: GPU recommended for faster training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply nest_asyncio for Jupyter compatibility\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from environment import PlaywrightEnv, create_default_config\n",
        "from data.offline_dataset import OfflineDatasetCollector, Episode\n",
        "from models.q_network import QNetwork\n",
        "from models.config import create_default_network_config\n",
        "from training.cql_trainer import CQLTrainer, CQLConfig\n",
        "from environment.utils import get_device\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Collect Offline Data\n",
        "\n",
        "First, we collect offline episodes using random and heuristic policies. This creates a diverse dataset with varying quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create OpenScope environment\n",
        "env = PlaywrightEnv(\n",
        "    airport=\"KLAS\",\n",
        "    max_aircraft=5,  # Small for faster data collection\n",
        "    headless=True,\n",
        "    timewarp=5,\n",
        "    episode_length=600,  # 10 minutes\n",
        ")\n",
        "\n",
        "print(\"âœ… Environment created\")\n",
        "\n",
        "# Initialize collector\n",
        "collector = OfflineDatasetCollector(env)\n",
        "\n",
        "# Collect random episodes\n",
        "print(\"\\nðŸ“Š Collecting offline data...\")\n",
        "print(\"   Random policy episodes...\")\n",
        "random_episodes = collector.collect_random_episodes(\n",
        "    num_episodes=100,  # Increase for real training\n",
        "    max_steps=100,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Collect heuristic episodes\n",
        "print(\"\\n   Heuristic policy episodes...\")\n",
        "heuristic_episodes = collector.collect_heuristic_episodes(\n",
        "    num_episodes=100,\n",
        "    max_steps=100,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Combine all episodes\n",
        "all_episodes = random_episodes + heuristic_episodes\n",
        "\n",
        "print(f\"\\nâœ… Collected {len(all_episodes)} episodes\")\n",
        "print(f\"   Total timesteps: {sum(ep.length for ep in all_episodes)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Train CQL Q-Network\n",
        "\n",
        "Now we train the CQL algorithm to learn a conservative Q-function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure CQL training\n",
        "config = CQLConfig(\n",
        "    max_aircraft=5,\n",
        "    num_epochs=50,  # Reduce for demo\n",
        "    batch_size=256,\n",
        "    learning_rate=3e-4,\n",
        "    cql_alpha=5.0,  # Conservative penalty weight\n",
        "    gamma=0.99,\n",
        "    checkpoint_dir=\"../checkpoints/cql\",\n",
        "    save_every=10,\n",
        "    device=get_device(),\n",
        ")\n",
        "\n",
        "print(\"ðŸ“‹ CQL Training Configuration:\")\n",
        "print(f\"   Max aircraft: {config.max_aircraft}\")\n",
        "print(f\"   Epochs: {config.num_epochs}\")\n",
        "print(f\"   Batch size: {config.batch_size}\")\n",
        "print(f\"   Learning rate: {config.learning_rate}\")\n",
        "print(f\"   CQL alpha (conservative penalty): {config.cql_alpha}\")\n",
        "print(f\"   Gamma (discount): {config.gamma}\")\n",
        "print(f\"   Device: {config.device}\")\n",
        "\n",
        "# Create trainer\n",
        "trainer = CQLTrainer(config)\n",
        "\n",
        "print(\"\\nâœ… CQL trainer created\")\n",
        "\n",
        "# Train CQL\n",
        "print(\"\\nðŸš€ Starting CQL training...\")\n",
        "print(\"   This learns a conservative Q-function from the offline dataset\")\n",
        "history = trainer.train(all_episodes)\n",
        "\n",
        "print(\"\\nâœ… CQL training complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
