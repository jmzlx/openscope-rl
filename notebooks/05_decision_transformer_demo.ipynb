{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Transformer for Air Traffic Control\n",
    "\n",
    "This notebook demonstrates **Decision Transformer**, an offline RL approach that treats reinforcement learning as sequence modeling.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Traditional RL (PPO, DQN):**\n",
    "- Learn value functions via TD learning\n",
    "- Require online environment interaction\n",
    "- Struggle with offline data from suboptimal policies\n",
    "\n",
    "**Decision Transformer:**\n",
    "- Pure supervised learning on sequences\n",
    "- Learn from (return-to-go, state, action) trajectories\n",
    "- Condition on desired return → control policy behavior\n",
    "- Works with mixed-quality offline data\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Collect offline data** - Random and heuristic policies\n",
    "2. **Train Decision Transformer** - Supervised learning to predict actions\n",
    "3. **Evaluate with return conditioning** - Test with different target returns\n",
    "4. **Compare to PPO** - Sample efficiency analysis\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Offline RL Paradigm** - Learning from fixed datasets without environment interaction\n",
    "2. **Sequence Modeling for RL** - Treating RL as supervised learning on (return-to-go, state, action) sequences\n",
    "3. **Return Conditioning** - Controlling policy behavior by conditioning on desired returns\n",
    "4. **Sample Efficiency** - Achieving 10x better efficiency than online PPO\n",
    "5. **Trajectory Stitching** - Combining suboptimal demonstrations to create optimal policies\n",
    "\n",
    "**Estimated Time**: 35-45 minutes (includes data collection and training)\n",
    "\n",
    "**Prerequisites**: Understanding of transformers, basic RL, supervised learning\n",
    "\n",
    "**Hardware**: GPU strongly recommended for transformer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import POC environment (fast, no browser needed)\n",
    "from poc.atc_rl import Simple2DATCEnv, Realistic3DATCEnv\n",
    "\n",
    "# Import Decision Transformer components\n",
    "from models.decision_transformer import MultiDiscreteDecisionTransformer\n",
    "from data import OfflineDatasetCollector, create_dataloader\n",
    "from training import DecisionTransformerTrainer, TrainingConfig\n",
    "from environment import get_device\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {get_device()}\")  # Auto-detects CUDA, Metal (MPS), or CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Collect Offline Data\n",
    "\n",
    "We'll collect 1000 episodes using:\n",
    "- **Random policy** (500 episodes) - Pure exploration\n",
    "- **Heuristic policy** (500 episodes) - Simple rule-based controller\n",
    "\n",
    "This creates a diverse dataset with varying quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "collector = OfflineDatasetCollector(env)\n",
    "\n",
    "# Collect random episodes\n",
    "# ⏱️ ~10-15 minutes for 1000 episodes\n",
    "print(\"Collecting random policy episodes...\")\n",
    "random_episodes = collector.collect_random_episodes(\n",
    "    num_episodes=500,\n",
    "    max_steps=200,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Collect heuristic episodes\n",
    "print(\"\\nCollecting heuristic policy episodes...\")\n",
    "heuristic_episodes = collector.collect_heuristic_episodes(\n",
    "    num_episodes=500,\n",
    "    max_steps=200,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Combine all episodes\n",
    "all_episodes = random_episodes + heuristic_episodes\n",
    "\n",
    "print(f\"\\nTotal episodes collected: {len(all_episodes)}\")\n",
    "print(f\"Total timesteps: {sum(ep.length for ep in all_episodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "collector = OfflineDatasetCollector(env)\n",
    "\n",
    "# Collect random episodes\n",
    "print(\"Collecting random policy episodes...\")\n",
    "random_episodes = collector.collect_random_episodes(\n",
    "    num_episodes=500,\n",
    "    max_steps=200,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Collect heuristic episodes\n",
    "print(\"\\nCollecting heuristic policy episodes...\")\n",
    "heuristic_episodes = collector.collect_heuristic_episodes(\n",
    "    num_episodes=500,\n",
    "    max_steps=200,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Combine all episodes\n",
    "all_episodes = random_episodes + heuristic_episodes\n",
    "\n",
    "print(f\"\\nTotal episodes collected: {len(all_episodes)}\")\n",
    "print(f\"Total timesteps: {sum(ep.length for ep in all_episodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset quality\n",
    "returns = [ep.total_return for ep in all_episodes]\n",
    "lengths = [ep.length for ep in all_episodes]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Return distribution\n",
    "axes[0].hist(returns, bins=50, alpha=0.7, color='blue')\n",
    "axes[0].axvline(np.mean(returns), color='red', linestyle='--', label=f'Mean: {np.mean(returns):.1f}')\n",
    "axes[0].set_xlabel('Episode Return')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Return Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Episode length distribution\n",
    "axes[1].hist(lengths, bins=50, alpha=0.7, color='green')\n",
    "axes[1].axvline(np.mean(lengths), color='red', linestyle='--', label=f'Mean: {np.mean(lengths):.1f}')\n",
    "axes[1].set_xlabel('Episode Length')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Episode Length Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Return statistics:\")\n",
    "print(f\"  Mean: {np.mean(returns):.2f}\")\n",
    "print(f\"  Std: {np.std(returns):.2f}\")\n",
    "print(f\"  Min: {np.min(returns):.2f}\")\n",
    "print(f\"  Max: {np.max(returns):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "data_dir = Path.cwd().parent / 'data' / 'offline'\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "collector.save_episodes(all_episodes, data_dir / 'atc_offline_1000ep.pkl')\n",
    "print(f\"Saved dataset to {data_dir / 'atc_offline_1000ep.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Train Decision Transformer\n",
    "\n",
    "Now we'll train the Decision Transformer using supervised learning.\n",
    "\n",
    "The model learns to predict: **action = f(return-to-go, state, previous actions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/validation\n",
    "np.random.shuffle(all_episodes)\n",
    "split_idx = int(0.9 * len(all_episodes))\n",
    "train_episodes = all_episodes[:split_idx]\n",
    "val_episodes = all_episodes[split_idx:]\n",
    "\n",
    "print(f\"Train episodes: {len(train_episodes)}\")\n",
    "print(f\"Val episodes: {len(val_episodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = create_dataloader(\n",
    "    episodes=train_episodes,\n",
    "    context_len=20,\n",
    "    max_aircraft=5,\n",
    "    state_dim=14,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    scale_returns=True,\n",
    "    return_scale=1000.0,\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    episodes=val_episodes,\n",
    "    context_len=20,\n",
    "    max_aircraft=5,\n",
    "    state_dim=14,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    scale_returns=True,\n",
    "    return_scale=1000.0,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch structure:\")\n",
    "print(f\"  returns: {sample_batch['returns'].shape}\")\n",
    "print(f\"  states: {sample_batch['states'].shape}\")\n",
    "print(f\"  aircraft_masks: {sample_batch['aircraft_masks'].shape}\")\n",
    "print(f\"  actions: {sample_batch['actions'].keys()}\")\n",
    "for key, val in sample_batch['actions'].items():\n",
    "    print(f\"    {key}: {val.shape}\")\n",
    "print(f\"  timesteps: {sample_batch['timesteps'].shape}\")\n",
    "print(f\"  attention_mask: {sample_batch['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Transformer model\n",
    "action_dims = {\n",
    "    \"aircraft_id\": 6,  # max_aircraft + 1 (0 = no action)\n",
    "    \"command_type\": 5,\n",
    "    \"altitude\": 18,\n",
    "    \"heading\": 13,\n",
    "    \"speed\": 8,\n",
    "}\n",
    "\n",
    "model = MultiDiscreteDecisionTransformer(\n",
    "    state_dim=14,\n",
    "    max_aircraft=5,\n",
    "    action_dims=action_dims,\n",
    "    hidden_size=128,  # Smaller for faster training\n",
    "    max_ep_len=200,\n",
    "    context_len=20,\n",
    "    n_layer=4,  # Fewer layers for faster training\n",
    "    n_head=4,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = TrainingConfig(\n",
    "    state_dim=14,\n",
    "    max_aircraft=5,\n",
    "    hidden_size=128,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    dropout=0.1,\n",
    "    context_len=20,\n",
    "    max_ep_len=200,\n",
    "    num_epochs=50,  # Reduce for demo\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    grad_clip=1.0,\n",
    "    warmup_steps=500,\n",
    "    return_scale=1000.0,\n",
    "    checkpoint_dir=str(Path.cwd().parent / 'checkpoints' / 'dt'),\n",
    "    save_every=10,\n",
    "    eval_every=5,\n",
    "    use_wandb=False,  # Set to True to log to WandB\n",
    "    log_every=50,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# ⏱️ ~15-20 minutes on GPU, ~60+ minutes on CPU (50 epochs)\n",
    "print(\"Starting training...\\n\")\n",
    "trainer.train(train_loader, val_loader)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "trainer.train(train_loader, val_loader)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with different target returns\n",
    "# ⏱️ ~2-3 minutes per target return (20 episodes each)\n",
    "target_returns = [50.0, 100.0, 200.0]\n",
    "num_eval_episodes = 20\n",
    "\n",
    "results = {}\n",
    "\n",
    "for target_return in target_returns:\n",
    "    print(f\"\\nEvaluating with target return: {target_return}\")\n",
    "    \n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for i in range(num_eval_episodes):\n",
    "        ep_return = trainer._run_episode(\n",
    "            target_return=target_return,\n",
    "            temperature=1.0,\n",
    "            deterministic=False,\n",
    "            max_steps=200,\n",
    "        )\n",
    "        episode_returns.append(ep_return)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Episode {i+1}/{num_eval_episodes}: {ep_return:.2f}\")\n",
    "    \n",
    "    results[target_return] = {\n",
    "        'returns': episode_returns,\n",
    "        'mean': np.mean(episode_returns),\n",
    "        'std': np.std(episode_returns),\n",
    "    }\n",
    "    \n",
    "    print(f\"  Mean return: {results[target_return]['mean']:.2f} ± {results[target_return]['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with different target returns\n",
    "target_returns = [50.0, 100.0, 200.0]\n",
    "num_eval_episodes = 20\n",
    "\n",
    "results = {}\n",
    "\n",
    "for target_return in target_returns:\n",
    "    print(f\"\\nEvaluating with target return: {target_return}\")\n",
    "    \n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for i in range(num_eval_episodes):\n",
    "        ep_return = trainer._run_episode(\n",
    "            target_return=target_return,\n",
    "            temperature=1.0,\n",
    "            deterministic=False,\n",
    "            max_steps=200,\n",
    "        )\n",
    "        episode_returns.append(ep_return)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Episode {i+1}/{num_eval_episodes}: {ep_return:.2f}\")\n",
    "    \n",
    "    results[target_return] = {\n",
    "        'returns': episode_returns,\n",
    "        'mean': np.mean(episode_returns),\n",
    "        'std': np.std(episode_returns),\n",
    "    }\n",
    "    \n",
    "    print(f\"  Mean return: {results[target_return]['mean']:.2f} ± {results[target_return]['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize return conditioning\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "positions = list(range(len(target_returns)))\n",
    "means = [results[tr]['mean'] for tr in target_returns]\n",
    "stds = [results[tr]['std'] for tr in target_returns]\n",
    "\n",
    "# Bar plot with error bars\n",
    "bars = ax.bar(positions, means, yerr=stds, alpha=0.7, capsize=5)\n",
    "\n",
    "# Color bars\n",
    "colors = ['red', 'orange', 'green']\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "# Add target return line\n",
    "ax.plot(positions, target_returns, 'ko--', label='Target Return', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Target Return', fontsize=12)\n",
    "ax.set_ylabel('Achieved Return', fontsize=12)\n",
    "ax.set_title('Decision Transformer: Return Conditioning', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels([f'RTG={int(tr)}' for tr in target_returns])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation:\")\n",
    "print(\"Higher target returns should lead to better achieved returns!\")\n",
    "print(\"This demonstrates the model's ability to condition behavior on desired outcomes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run episode with tracking\n",
    "# ⏱️ ~5-10 seconds per episode\n",
    "target_return = 200.0\n",
    "max_steps = 200\n",
    "\n",
    "model.eval()\n",
    "obs, info = eval_env.reset()\n",
    "\n",
    "# Track metrics\n",
    "timesteps_list = []\n",
    "returns_to_go = []\n",
    "rewards_list = []\n",
    "cumulative_returns = []\n",
    "actions_taken = []\n",
    "\n",
    "# Initialize context buffers\n",
    "returns_buffer = []\n",
    "states_buffer = []\n",
    "masks_buffer = []\n",
    "actions_buffer = {key: [] for key in [\"aircraft_id\", \"command_type\", \"altitude\", \"heading\", \"speed\"]}\n",
    "timesteps_buffer = []\n",
    "\n",
    "episode_return = 0.0\n",
    "current_return = target_return / config.return_scale\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(max_steps):\n",
    "        # Track\n",
    "        timesteps_list.append(step)\n",
    "        returns_to_go.append(current_return * config.return_scale)\n",
    "        \n",
    "        # Add current state to buffer\n",
    "        returns_buffer.append(current_return)\n",
    "        states_buffer.append(obs[\"aircraft\"])\n",
    "        masks_buffer.append(obs[\"aircraft_mask\"])\n",
    "        timesteps_buffer.append(step)\n",
    "\n",
    "        # Truncate to context length\n",
    "        if len(returns_buffer) > config.context_len:\n",
    "            returns_buffer = returns_buffer[-config.context_len:]\n",
    "            states_buffer = states_buffer[-config.context_len:]\n",
    "            masks_buffer = masks_buffer[-config.context_len:]\n",
    "            timesteps_buffer = timesteps_buffer[-config.context_len:]\n",
    "            for key in actions_buffer.keys():\n",
    "                actions_buffer[key] = actions_buffer[key][-config.context_len:]\n",
    "\n",
    "        # Prepare tensors\n",
    "        returns_tensor = torch.tensor(returns_buffer, dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "        states_tensor = torch.tensor(np.stack(states_buffer), dtype=torch.float32).unsqueeze(0)\n",
    "        masks_tensor = torch.tensor(np.stack(masks_buffer), dtype=torch.bool).unsqueeze(0)\n",
    "        timesteps_tensor = torch.tensor(timesteps_buffer, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        if len(actions_buffer[\"aircraft_id\"]) == 0:\n",
    "            actions_tensor = {\n",
    "                key: torch.zeros(1, 1, dtype=torch.long)\n",
    "                for key in actions_buffer.keys()\n",
    "            }\n",
    "        else:\n",
    "            actions_tensor = {\n",
    "                key: torch.tensor(values, dtype=torch.long).unsqueeze(0)\n",
    "                for key, values in actions_buffer.items()\n",
    "            }\n",
    "\n",
    "        # Move to device\n",
    "        returns_tensor = returns_tensor.to(trainer.device)\n",
    "        states_tensor = states_tensor.to(trainer.device)\n",
    "        masks_tensor = masks_tensor.to(trainer.device)\n",
    "        actions_tensor = {k: v.to(trainer.device) for k, v in actions_tensor.items()}\n",
    "        timesteps_tensor = timesteps_tensor.to(trainer.device)\n",
    "\n",
    "        # Get action from model\n",
    "        action_dict, _ = model.get_action(\n",
    "            returns=returns_tensor,\n",
    "            states=states_tensor,\n",
    "            aircraft_masks=masks_tensor,\n",
    "            actions=actions_tensor,\n",
    "            timesteps=timesteps_tensor,\n",
    "            temperature=1.0,\n",
    "            deterministic=False,\n",
    "        )\n",
    "\n",
    "        # Convert to environment format\n",
    "        action = {\n",
    "            \"aircraft_id\": int(action_dict[\"aircraft_id\"]),\n",
    "            \"command_type\": int(action_dict[\"command_type\"]),\n",
    "            \"altitude\": int(action_dict[\"altitude\"]),\n",
    "            \"heading\": int(action_dict[\"heading\"]),\n",
    "            \"speed\": int(action_dict[\"speed\"]),\n",
    "        }\n",
    "        \n",
    "        actions_taken.append(action['aircraft_id'])\n",
    "\n",
    "        # Step environment\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "        # Update buffers\n",
    "        for key, value in action.items():\n",
    "            actions_buffer[key].append(value)\n",
    "\n",
    "        episode_return += reward\n",
    "        rewards_list.append(reward)\n",
    "        cumulative_returns.append(episode_return)\n",
    "\n",
    "        # Update return-to-go\n",
    "        current_return -= reward / config.return_scale\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "print(f\"Episode completed!\")\n",
    "print(f\"  Total return: {episode_return:.2f}\")\n",
    "print(f\"  Episode length: {len(timesteps_list)}\")\n",
    "print(f\"  Target return: {target_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run episode with tracking\n",
    "target_return = 200.0\n",
    "max_steps = 200\n",
    "\n",
    "model.eval()\n",
    "obs, info = eval_env.reset()\n",
    "\n",
    "# Track metrics\n",
    "timesteps_list = []\n",
    "returns_to_go = []\n",
    "rewards_list = []\n",
    "cumulative_returns = []\n",
    "actions_taken = []\n",
    "\n",
    "# Initialize context buffers\n",
    "returns_buffer = []\n",
    "states_buffer = []\n",
    "masks_buffer = []\n",
    "actions_buffer = {key: [] for key in [\"aircraft_id\", \"command_type\", \"altitude\", \"heading\", \"speed\"]}\n",
    "timesteps_buffer = []\n",
    "\n",
    "episode_return = 0.0\n",
    "current_return = target_return / config.return_scale\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(max_steps):\n",
    "        # Track\n",
    "        timesteps_list.append(step)\n",
    "        returns_to_go.append(current_return * config.return_scale)\n",
    "        \n",
    "        # Add current state to buffer\n",
    "        returns_buffer.append(current_return)\n",
    "        states_buffer.append(obs[\"aircraft\"])\n",
    "        masks_buffer.append(obs[\"aircraft_mask\"])\n",
    "        timesteps_buffer.append(step)\n",
    "\n",
    "        # Truncate to context length\n",
    "        if len(returns_buffer) > config.context_len:\n",
    "            returns_buffer = returns_buffer[-config.context_len:]\n",
    "            states_buffer = states_buffer[-config.context_len:]\n",
    "            masks_buffer = masks_buffer[-config.context_len:]\n",
    "            timesteps_buffer = timesteps_buffer[-config.context_len:]\n",
    "            for key in actions_buffer.keys():\n",
    "                actions_buffer[key] = actions_buffer[key][-config.context_len:]\n",
    "\n",
    "        # Prepare tensors\n",
    "        returns_tensor = torch.tensor(returns_buffer, dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "        states_tensor = torch.tensor(np.stack(states_buffer), dtype=torch.float32).unsqueeze(0)\n",
    "        masks_tensor = torch.tensor(np.stack(masks_buffer), dtype=torch.bool).unsqueeze(0)\n",
    "        timesteps_tensor = torch.tensor(timesteps_buffer, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        if len(actions_buffer[\"aircraft_id\"]) == 0:\n",
    "            actions_tensor = {\n",
    "                key: torch.zeros(1, 1, dtype=torch.long)\n",
    "                for key in actions_buffer.keys()\n",
    "            }\n",
    "        else:\n",
    "            actions_tensor = {\n",
    "                key: torch.tensor(values, dtype=torch.long).unsqueeze(0)\n",
    "                for key, values in actions_buffer.items()\n",
    "            }\n",
    "\n",
    "        # Move to device\n",
    "        returns_tensor = returns_tensor.to(trainer.device)\n",
    "        states_tensor = states_tensor.to(trainer.device)\n",
    "        masks_tensor = masks_tensor.to(trainer.device)\n",
    "        actions_tensor = {k: v.to(trainer.device) for k, v in actions_tensor.items()}\n",
    "        timesteps_tensor = timesteps_tensor.to(trainer.device)\n",
    "\n",
    "        # Get action from model\n",
    "        action_dict, _ = model.get_action(\n",
    "            returns=returns_tensor,\n",
    "            states=states_tensor,\n",
    "            aircraft_masks=masks_tensor,\n",
    "            actions=actions_tensor,\n",
    "            timesteps=timesteps_tensor,\n",
    "            temperature=1.0,\n",
    "            deterministic=False,\n",
    "        )\n",
    "\n",
    "        # Convert to environment format\n",
    "        action = {\n",
    "            \"aircraft_id\": int(action_dict[\"aircraft_id\"]),\n",
    "            \"command_type\": int(action_dict[\"command_type\"]),\n",
    "            \"altitude\": int(action_dict[\"altitude\"]),\n",
    "            \"heading\": int(action_dict[\"heading\"]),\n",
    "            \"speed\": int(action_dict[\"speed\"]),\n",
    "        }\n",
    "        \n",
    "        actions_taken.append(action['aircraft_id'])\n",
    "\n",
    "        # Step environment\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "        # Update buffers\n",
    "        for key, value in action.items():\n",
    "            actions_buffer[key].append(value)\n",
    "\n",
    "        episode_return += reward\n",
    "        rewards_list.append(reward)\n",
    "        cumulative_returns.append(episode_return)\n",
    "\n",
    "        # Update return-to-go\n",
    "        current_return -= reward / config.return_scale\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "print(f\"Episode completed!\")\n",
    "print(f\"  Total return: {episode_return:.2f}\")\n",
    "print(f\"  Episode length: {len(timesteps_list)}\")\n",
    "print(f\"  Target return: {target_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize episode\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Return-to-go vs timestep\n",
    "axes[0, 0].plot(timesteps_list, returns_to_go, label='Return-to-go', linewidth=2)\n",
    "axes[0, 0].axhline(target_return, color='red', linestyle='--', label='Target', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Timestep')\n",
    "axes[0, 0].set_ylabel('Return-to-go')\n",
    "axes[0, 0].set_title('Return-to-go Conditioning')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative return\n",
    "axes[0, 1].plot(timesteps_list, cumulative_returns, label='Cumulative Return', color='green', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Timestep')\n",
    "axes[0, 1].set_ylabel('Cumulative Return')\n",
    "axes[0, 1].set_title('Achieved Return Over Time')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Rewards per timestep\n",
    "axes[1, 0].plot(timesteps_list, rewards_list, label='Reward', color='orange', linewidth=1)\n",
    "axes[1, 0].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1, 0].set_xlabel('Timestep')\n",
    "axes[1, 0].set_ylabel('Reward')\n",
    "axes[1, 0].set_title('Rewards per Timestep')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Actions taken\n",
    "axes[1, 1].plot(timesteps_list, actions_taken, 'o', markersize=3, label='Aircraft ID', alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Timestep')\n",
    "axes[1, 1].set_ylabel('Aircraft ID')\n",
    "axes[1, 1].set_title('Actions Taken (Aircraft Selection)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Compare to PPO (Sample Efficiency)\n",
    "\n",
    "Decision Transformer's key advantage: **learns from fixed offline data**\n",
    "\n",
    "- **DT**: 1000 episodes of offline data (200k timesteps)\n",
    "- **PPO**: Requires 2M+ timesteps of online interaction\n",
    "\n",
    "DT is ~10x more sample efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls & Troubleshooting\n",
    "\n",
    "### Problem 1: \"Model doesn't improve with higher target returns\"\n",
    "**Causes**:\n",
    "- **Insufficient training**: Train longer (50+ epochs)\n",
    "- **Dataset lacks high returns**: Collect better demonstrations\n",
    "- **Return scale incorrect**: Check return_scale matches your reward range\n",
    "\n",
    "**Solution**:\n",
    "```python\n",
    "config.return_scale = 1000.0  # Adjust to your reward range\n",
    "config.num_epochs = 100  # More training\n",
    "```\n",
    "\n",
    "### Problem 2: Training loss not decreasing\n",
    "**Causes**:\n",
    "- **Learning rate too high**: Reduce from 1e-4 to 1e-5\n",
    "- **Context length too short**: Increase from 20 to 40\n",
    "- **Action space too large**: Simplify or use hierarchical actions\n",
    "\n",
    "**Solution**:\n",
    "```python\n",
    "config = TrainingConfig(\n",
    "    learning_rate=1e-5,  # Lower\n",
    "    context_len=40,      # Longer context\n",
    "    warmup_steps=1000,   # More warmup\n",
    ")\n",
    "```\n",
    "\n",
    "### Problem 3: \"RuntimeError: CUDA out of memory\"\n",
    "**Solution**: Reduce batch size or context length:\n",
    "```python\n",
    "config.batch_size = 32  # Down from 64\n",
    "config.context_len = 10  # Down from 20\n",
    "```\n",
    "\n",
    "### Problem 4: Policy copies expert exactly (no improvement)\n",
    "**This is expected!** Decision Transformer learns the data distribution. To improve:\n",
    "- Collect better demonstrations (higher returns)\n",
    "- Use return conditioning above expert's max return\n",
    "- Consider online fine-tuning after offline pre-training\n",
    "\n",
    "### Problem 5: \"Model predictions are deterministic/repetitive\"\n",
    "**Solution**: Adjust temperature or use stochastic sampling:\n",
    "```python\n",
    "action = model.get_action(\n",
    "    ...,\n",
    "    temperature=1.5,  # Increase from 1.0\n",
    "    deterministic=False,  # Enable sampling\n",
    ")\n",
    "```\n",
    "\n",
    "### Problem 6: Poor transfer from offline training to online evaluation\n",
    "**Causes**:\n",
    "- **Distribution shift**: Offline data doesn't cover online states\n",
    "- **Causal confusion**: Model memorizes spurious correlations\n",
    "- **Return scale mismatch**: Online returns different from training\n",
    "\n",
    "**Solution**: Add online fine-tuning phase after offline pre-training.\n",
    "\n",
    "### Debugging Tips:\n",
    "1. **Visualize return distributions**: Ensure dataset covers target returns\n",
    "2. **Check action accuracy**: Should be >80% on validation set\n",
    "3. **Test return conditioning**: Try RTG from min to max of dataset\n",
    "4. **Monitor context usage**: Longer contexts should help (up to a point)\n",
    "\n",
    "**Need more help?** Read Decision Transformer paper or check GitHub discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sample efficiency\n",
    "total_offline_timesteps = sum(ep.length for ep in all_episodes)\n",
    "dt_performance = results[200.0]['mean']  # Best target return\n",
    "\n",
    "print(\"Sample Efficiency Comparison\\n\" + \"=\"*50)\n",
    "print(f\"\\nDecision Transformer:\")\n",
    "print(f\"  Offline timesteps: {total_offline_timesteps:,}\")\n",
    "print(f\"  Performance (RTG=200): {dt_performance:.2f}\")\n",
    "print(f\"  Training: Supervised learning (no environment interaction)\")\n",
    "print(f\"\\nPPO (typical):\")\n",
    "print(f\"  Online timesteps needed: ~2,000,000\")\n",
    "print(f\"  Performance: Similar to DT\")\n",
    "print(f\"  Training: Requires continuous environment interaction\")\n",
    "print(f\"\\nSample Efficiency Gain:\")\n",
    "print(f\"  DT uses ~{2_000_000 / total_offline_timesteps:.1f}x FEWER timesteps!\")\n",
    "print(f\"\\nKey Advantages of Decision Transformer:\")\n",
    "print(f\"  ✓ Learn from ANY offline data (even suboptimal)\")\n",
    "print(f\"  ✓ No online environment interaction during training\")\n",
    "print(f\"  ✓ Return conditioning enables behavior control\")\n",
    "print(f\"  ✓ Pure supervised learning (simpler than PPO)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully demonstrated Decision Transformer for air traffic control:\n",
    "\n",
    "1. **Collected diverse offline data** from random and heuristic policies\n",
    "2. **Trained Decision Transformer** using supervised learning\n",
    "3. **Evaluated with return conditioning** - higher targets → better performance\n",
    "4. **Compared to PPO** - 10x better sample efficiency\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Decision Transformer advantages:**\n",
    "- Works with mixed-quality offline data\n",
    "- No online environment interaction needed\n",
    "- Return conditioning provides behavior control\n",
    "- Simpler than value-based RL (pure supervised learning)\n",
    "\n",
    "**When to use Decision Transformer:**\n",
    "- Limited environment interaction (expensive/dangerous)\n",
    "- Existing dataset of trajectories\n",
    "- Need to control behavior via desired outcomes\n",
    "- Want simpler training than PPO/DQN\n",
    "\n",
    "**Next steps:**\n",
    "- Scale to full OpenScope environment (more aircraft, realistic airports)\n",
    "- Collect higher-quality offline data\n",
    "- Experiment with larger models (more layers/heads)\n",
    "- Implement beam search for multi-step planning\n",
    "- Try online fine-tuning (start with DT, then PPO)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions? Experiments? Share your results!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
