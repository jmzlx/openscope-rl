{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DreamerV3 for Model-Based RL in OpenScope\n",
        "\n",
        "This notebook demonstrates a simplified DreamerV3-style workflow: learn a latent world model and train a policy from imagined trajectories for sample-efficient RL.\n",
        "\n",
        "## Why DreamerV3?\n",
        "- Model-based RL with imagination enables much higher sample efficiency\n",
        "- Joint training of world model, actor, and critic\n",
        "- Strong empirical results across control tasks\n",
        "\n",
        "## Workflow\n",
        "1. Load/collect offline trajectories from OpenScope\n",
        "2. Train world model (encoder + RSSM + decoders)\n",
        "3. Train actor/critic with imagined rollouts\n",
        "4. Evaluate policy in OpenScope\n",
        "\n",
        "## Prerequisites\n",
        "- Offline dataset (or OpenScope server for collection)\n",
        "- GPU recommended\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note on External Library\n",
        "\n",
        "This demo uses an external implementation of DreamerV3 (TorchRL) to keep the notebook focused on OpenScope integration rather than re-implementing the algorithm. In production, you can pin exact versions and move the config into code.\n",
        "\n",
        "- Library: TorchRL (DreamerV3 implementation)\n",
        "- Install: see the next cell for a one-time install command\n",
        "\n",
        "### Production Config (recommended starting point)\n",
        "- World model: latent_dim=64, rssm_hidden=200, rssm_stochastic=32, rssm_discrete=False\n",
        "- Imagination: horizon=15‚Äì50, gamma=0.997\n",
        "- Optimization: actor_lr=3e-4, critic_lr=3e-4, model_lr=3e-4, batch_size=512\n",
        "- Replay: capacity=1e6 transitions, prefill=5e4 random steps\n",
        "- Training cadence: update_every=16 env steps, updates_per_step=1.0\n",
        "- Parallelism: 8‚Äì16 envs, collector on separate process\n",
        "- Logging: WandB/TensorBoard, checkpoints every 10k updates\n",
        "\n",
        "## Summary\n",
        "\n",
        "- This notebook outlines a DreamerV3-style training flow using TorchRL.\n",
        "- For production, switch to the provided production config and use TorchRL collectors + trainers.\n",
        "- Integrate the policy with `PlaywrightEnv` for evaluation once trained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Install TorchRL with DreamerV3 implementation\n",
        "# Uncomment if not already installed in your environment\n",
        "# %pip install torchrl==0.5.0 torch>=2.2.0 torchvision --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from environment.utils import get_device\n",
        "\n",
        "print(\"‚úÖ Environment ready\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Load or Collect Trajectories\n",
        "\n",
        "We will use the same offline data format as other notebooks. If none found, collect a small set via Playwright (slower).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from data.offline_dataset import OfflineDatasetCollector\n",
        "\n",
        "# Try loading a pre-collected dataset\n",
        "DATA_PATH = Path(\"../data/offline_data.pkl\")\n",
        "\n",
        "if DATA_PATH.exists():\n",
        "    print(f\"üì¶ Loading episodes from {DATA_PATH}\")\n",
        "    episodes = OfflineDatasetCollector.load_episodes(str(DATA_PATH))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No dataset at ../data/offline_data.pkl. You can collect a small set below (slower).\")\n",
        "    episodes = []\n",
        "\n",
        "print(f\"Episodes: {len(episodes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: small data collection via Playwright (random policy)\n",
        "from environment import PlaywrightEnv\n",
        "\n",
        "if len(episodes) == 0:\n",
        "    try:\n",
        "        print(\"üé¨ Collecting 20 episodes (demo)\")\n",
        "        env = PlaywrightEnv(headless=True, timewarp=5, max_aircraft=5, episode_length=300)\n",
        "        collector = OfflineDatasetCollector(env)\n",
        "        episodes = collector.collect_random_episodes(num_episodes=20, max_steps=100, verbose=True)\n",
        "        print(f\"Collected {len(episodes)} episodes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Data collection failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Build Replay Buffer (TorchRL format)\n",
        "\n",
        "Convert the episodes into a replay buffer of (s, a, r, s', done).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.data import ReplayBuffer, ListStorage\n",
        "\n",
        "if len(episodes) == 0:\n",
        "    raise RuntimeError(\"No episodes available. Please load or collect data above.\")\n",
        "\n",
        "# Build a simple replay buffer\n",
        "rb = ReplayBuffer(storage=ListStorage())\n",
        "\n",
        "count = 0\n",
        "for ep in episodes:\n",
        "    for t in range(ep.length - 1):\n",
        "        s = ep.observations[t]\n",
        "        a = ep.actions[t]\n",
        "        r = ep.rewards[t]\n",
        "        s_next = ep.observations[t+1]\n",
        "        d = ep.dones[t]\n",
        "        rb.add({\n",
        "            'state': s,\n",
        "            'action': a,\n",
        "            'reward': r,\n",
        "            'next_state': s_next,\n",
        "            'done': d,\n",
        "        })\n",
        "        count += 1\n",
        "\n",
        "print(f\"‚úÖ ReplayBuffer filled with {count} transitions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Configure DreamerV3 (Demo vs Production)\n",
        "\n",
        "We define a small demo config for quick runs and a production config you can switch to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_cfg = {\n",
        "    'latent_dim': 32,\n",
        "    'rssm_hidden': 128,\n",
        "    'rssm_stochastic': 16,\n",
        "    'rssm_discrete': False,\n",
        "    'imagination_horizon': 15,\n",
        "    'gamma': 0.997,\n",
        "    'actor_lr': 3e-4,\n",
        "    'critic_lr': 3e-4,\n",
        "    'model_lr': 3e-4,\n",
        "    'batch_size': 256,\n",
        "}\n",
        "\n",
        "prod_cfg = {\n",
        "    'latent_dim': 64,\n",
        "    'rssm_hidden': 200,\n",
        "    'rssm_stochastic': 32,\n",
        "    'rssm_discrete': False,\n",
        "    'imagination_horizon': 30,\n",
        "    'gamma': 0.997,\n",
        "    'actor_lr': 3e-4,\n",
        "    'critic_lr': 3e-4,\n",
        "    'model_lr': 3e-4,\n",
        "    'batch_size': 512,\n",
        "}\n",
        "\n",
        "cfg = demo_cfg\n",
        "print(\"Using DreamerV3 config:\", cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: DreamerV3 Setup (TorchRL)\n",
        "\n",
        "Set up DreamerV3 components. This is a minimal sketch; see TorchRL docs for advanced usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal DreamerV3 setup with TorchRL (pseudo-API for clarity)\n",
        "from torchrl.objectives import DreamerV3Loss\n",
        "from torchrl.modules import RSSM\n",
        "\n",
        "# Build a small RSSM world model (encoder/decoder omitted for brevity)\n",
        "rssm = RSSM(\n",
        "    hidden_dim=cfg['rssm_hidden'],\n",
        "    stochastic_dim=cfg['rssm_stochastic'],\n",
        "    discrete=False,\n",
        ")\n",
        "\n",
        "loss_module = DreamerV3Loss(\n",
        "    rssm=rssm,\n",
        "    actor_lr=cfg['actor_lr'],\n",
        "    critic_lr=cfg['critic_lr'],\n",
        "    model_lr=cfg['model_lr'],\n",
        "    gamma=cfg['gamma'],\n",
        "    imagination_horizon=cfg['imagination_horizon'],\n",
        ")\n",
        "\n",
        "print(\"‚úÖ DreamerV3 components created (demo)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Train World Model from Offline Data\n",
        "\n",
        "Train the RSSM/world model on replay buffer (state transitions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "\n",
        "num_epochs = 5  # demo; increase for real training\n",
        "batch_size = cfg['batch_size']\n",
        "\n",
        "print(\"üöÄ Training world model (demo)‚Ä¶\")\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    # Simple manual batching from replay buffer storage\n",
        "    storage = rb._storage._storage  # ListStorage\n",
        "    np.random.shuffle(storage)\n",
        "    for i in trange(0, len(storage), batch_size, leave=False):\n",
        "        batch = storage[i:i+batch_size]\n",
        "        # Convert batch to tensors and compute DreamerV3 losses (model/actor/critic)\n",
        "        # NOTE: For brevity, this is a placeholder. Use TorchRL DataSpec + collectors in production.\n",
        "        # loss = loss_module(batch)\n",
        "        # loss.backward(); optimizer.step(); optimizer.zero_grad()\n",
        "        pass\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} done\")\n",
        "\n",
        "print(\"‚úÖ World model training (demo) complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Imagined Rollouts and Policy Training (Sketch)\n",
        "\n",
        "Use the learned world model to generate imagined trajectories and update actor/critic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pseudo-code: imagined rollouts and policy updates\n",
        "# for update in range(1000):\n",
        "#     posterior = rssm.observe(replay_batch)\n",
        "#     imagined_latents = rssm.imagine(posterior, horizon=cfg['imagination_horizon'])\n",
        "#     actor_loss, critic_loss = loss_module.actor_critic_losses(imagined_latents)\n",
        "#     (actor_loss + critic_loss).backward()\n",
        "#     actor_opt.step(); critic_opt.step(); actor_opt.zero_grad(); critic_opt.zero_grad()\n",
        "\n",
        "print(\"‚ÑπÔ∏è Imagined rollouts & policy training sketched. See TorchRL DreamerV3 examples for full training loops.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Evaluation in OpenScope (Outline)\n",
        "\n",
        "After training, evaluate the policy in `PlaywrightEnv`:\n",
        "\n",
        "```python\n",
        "from environment import PlaywrightEnv\n",
        "\n",
        "env = PlaywrightEnv(headless=True, timewarp=5, max_aircraft=5, episode_length=600)\n",
        "obs, info = env.reset()\n",
        "\n",
        "total_reward = 0\n",
        "for t in range(120):  # ~10 minutes at 5s interval\n",
        "    # Use actor to sample action given latent state (pseudo)\n",
        "    # action = actor.sample(obs)\n",
        "    # For demo, sample random:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "print(\"Episode reward:\", total_reward)\n",
        "```\n",
        "\n",
        "Replace the random action with the DreamerV3 actor once trained end-to-end.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References and Version Pinning\n",
        "\n",
        "- TorchRL DreamerV3 tutorial: https://pytorch.org/torchrl/stable/tutorials/foundations/dreamer_v3.html\n",
        "- Paper: DreamerV3 (Hafner et al.)\n",
        "\n",
        "Recommended pins (example):\n",
        "\n",
        "```bash\n",
        "pip install \"torch>=2.2,<2.4\" torchvision \\\n",
        "  torchrl==0.5.0 tensordict==0.5.0 \\\n",
        "  --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "```\n",
        "\n",
        "Switch `cfg = prod_cfg` above for production training."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
