{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent RL for Air Traffic Control\n",
    "\n",
    "This notebook demonstrates multi-agent reinforcement learning where each aircraft is an independent cooperative agent.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Multi-Agent Formulation:**\n",
    "- Each aircraft = independent agent with shared policy\n",
    "- Variable number of agents (aircraft spawn and exit dynamically)\n",
    "- Agents communicate via attention mechanism\n",
    "- Cooperative objective: maximize team reward\n",
    "\n",
    "**MAPPO Architecture:**\n",
    "- **Centralized Critic**: Sees global state (all aircraft + environment)\n",
    "- **Decentralized Actors**: See local observations + communication\n",
    "- **Communication**: Self-attention between agents enables coordination\n",
    "- **Training**: Centralized training, decentralized execution (CTDE)\n",
    "\n",
    "**Emergent Behaviors:**\n",
    "- Coordination patterns emerge from communication\n",
    "- Agents learn to share responsibility\n",
    "- Conflict avoidance through implicit negotiation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üìö Learning Objectives\n\nBy the end of this notebook, you will understand:\n\n1. **Multi-Agent Formulation** - Each aircraft as independent cooperative agent with shared policy\n2. **CTDE (Centralized Training, Decentralized Execution)** - Centralized critic, decentralized actors\n3. **Agent Communication** - Self-attention mechanism enabling coordination between aircraft\n4. **Emergent Coordination** - How cooperative behaviors emerge from learned communication\n5. **Scalability** - Parameter count doesn't grow with aircraft count (shared policy advantage)\n\n**Estimated Time**: 15-20 minutes (demonstration only, no training)\n**Prerequisites**: Understanding of RL, attention mechanisms, multi-agent concepts helpful\n**Hardware**: CPU sufficient for forward passes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from models import MultiAgentPolicy, create_default_network_config\n",
    "from training import MAPPOTrainer, MAPPOConfig\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Notebook settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Agent Policy Architecture\n",
    "\n",
    "Let's create and inspect the multi-agent policy architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network configuration\n",
    "config = create_default_network_config(\n",
    "    max_aircraft=10,\n",
    "    aircraft_feature_dim=14,\n",
    "    global_feature_dim=4,\n",
    "    hidden_dim=256,\n",
    "    num_encoder_layers=4,\n",
    "    num_attention_heads=8\n",
    ")\n",
    "\n",
    "# Create multi-agent policy\n",
    "policy = MultiAgentPolicy(config)\n",
    "\n",
    "# Print architecture\n",
    "print(\"Multi-Agent Policy Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(policy)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Parameter counts\n",
    "params = policy.count_parameters()\n",
    "print(\"\\nParameter Breakdown:\")\n",
    "print(f\"  Total Parameters: {params['total_parameters']:,}\")\n",
    "print(f\"  Encoder: {params['encoder_parameters']:,}\")\n",
    "print(f\"  Communication: {params['communication_parameters']:,}\")\n",
    "print(f\"  Actor: {params['actor_parameters']:,}\")\n",
    "print(f\"  Critic: {params['critic_parameters']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Pass Example\n",
    "\n",
    "Let's see how the policy processes observations and produces actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ‚è±Ô∏è ~5 seconds\n\n# Create sample observation\nbatch_size = 2\nnum_aircraft = 5\n\nobs = {\n    \"aircraft\": torch.randn(batch_size, 10, 14),  # All aircraft features\n    \"aircraft_mask\": torch.zeros(batch_size, 10, dtype=torch.bool),\n    \"global_state\": torch.randn(batch_size, 4)\n}\n\n# Set first 5 aircraft as active\nobs[\"aircraft_mask\"][:, :num_aircraft] = True\n\n# Forward pass\nwith torch.no_grad():\n    action_logits, value, comm_attention = policy(obs, return_communication=True)\n\nprint(\"\\nForward Pass Results:\")\nprint(\"=\" * 60)\nprint(f\"\\nAction Logits:\")\nfor key, logits in action_logits.items():\n    print(f\"  {key}: {logits.shape}\")\nprint(f\"\\nValue: {value.shape}\")\nprint(f\"\\nCommunication Attention Layers: {len(comm_attention)}\")\nif comm_attention:\n    print(f\"  Each layer shape: {comm_attention[0].shape}\")\n    print(f\"  Format: (batch_size, num_heads, num_agents, num_agents)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Communication Visualization\n",
    "\n",
    "Visualize attention patterns to understand agent communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_communication(attention_weights, agent_mask, layer_idx=0, head_idx=0, batch_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize communication attention between agents.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: List of attention weight tensors\n",
    "        agent_mask: Boolean mask indicating active agents\n",
    "        layer_idx: Which communication layer to visualize\n",
    "        head_idx: Which attention head to visualize\n",
    "        batch_idx: Which batch item to visualize\n",
    "    \"\"\"\n",
    "    # Get attention for specified layer/head/batch\n",
    "    attn = attention_weights[layer_idx][batch_idx, head_idx].cpu().numpy()\n",
    "    \n",
    "    # Get active agents\n",
    "    active = agent_mask[batch_idx].cpu().numpy()\n",
    "    num_active = active.sum()\n",
    "    \n",
    "    # Filter to only active agents\n",
    "    attn_active = attn[:num_active, :num_active]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Heatmap\n",
    "    sns.heatmap(\n",
    "        attn_active,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='YlOrRd',\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Attention Weight'},\n",
    "        ax=ax1\n",
    "    )\n",
    "    ax1.set_xlabel('Key Agent')\n",
    "    ax1.set_ylabel('Query Agent')\n",
    "    ax1.set_title(f'Communication Attention (Layer {layer_idx}, Head {head_idx})')\n",
    "    \n",
    "    # Communication graph\n",
    "    positions = np.array([[np.cos(2*np.pi*i/num_active), np.sin(2*np.pi*i/num_active)] \n",
    "                          for i in range(num_active)])\n",
    "    \n",
    "    # Draw nodes\n",
    "    ax2.scatter(positions[:, 0], positions[:, 1], s=500, c='skyblue', edgecolors='black', zorder=3)\n",
    "    \n",
    "    # Label nodes\n",
    "    for i, pos in enumerate(positions):\n",
    "        ax2.text(pos[0], pos[1], f'A{i}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw edges (communication links)\n",
    "    threshold = 0.1  # Only show strong attention\n",
    "    for i in range(num_active):\n",
    "        for j in range(num_active):\n",
    "            if i != j and attn_active[i, j] > threshold:\n",
    "                # Draw arrow from j to i (j is key, i is query)\n",
    "                dx = positions[i, 0] - positions[j, 0]\n",
    "                dy = positions[i, 1] - positions[j, 1]\n",
    "                \n",
    "                # Scale arrow by attention weight\n",
    "                alpha = min(attn_active[i, j] * 2, 1.0)\n",
    "                width = attn_active[i, j] * 5\n",
    "                \n",
    "                ax2.arrow(\n",
    "                    positions[j, 0], positions[j, 1],\n",
    "                    dx * 0.8, dy * 0.8,\n",
    "                    width=width,\n",
    "                    head_width=0.1,\n",
    "                    head_length=0.1,\n",
    "                    alpha=alpha,\n",
    "                    color='red',\n",
    "                    zorder=2\n",
    "                )\n",
    "    \n",
    "    ax2.set_xlim(-1.5, 1.5)\n",
    "    ax2.set_ylim(-1.5, 1.5)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Communication Graph\\n(arrows show attention flow)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize communication\n",
    "if comm_attention:\n",
    "    visualize_communication(comm_attention, obs[\"aircraft_mask\"], layer_idx=0, head_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with MAPPO\n",
    "\n",
    "Set up and run MAPPO training (requires environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This requires a working ATC environment\n",
    "# Uncomment and modify the following to train:\n",
    "\n",
    "# from environment import PlaywrightEnv, create_default_config\n",
    "\n",
    "# # Create environment\n",
    "# env_config = create_default_config(\n",
    "#     airport=\"KSFO\",\n",
    "#     max_aircraft=10,\n",
    "#     headless=True,\n",
    "#     timewarp=10\n",
    "# )\n",
    "# env = PlaywrightEnv(**env_config.__dict__)\n",
    "\n",
    "# # Create training config\n",
    "# train_config = MAPPOConfig(\n",
    "#     max_aircraft=10,\n",
    "#     hidden_dim=256,\n",
    "#     learning_rate=3e-4,\n",
    "#     total_timesteps=100_000,\n",
    "#     steps_per_rollout=2048,\n",
    "#     log_dir=\"logs/mappo_demo\",\n",
    "#     save_dir=\"checkpoints/mappo_demo\"\n",
    "# )\n",
    "\n",
    "# # Create trainer\n",
    "# trainer = MAPPOTrainer(env, train_config)\n",
    "\n",
    "# # Train\n",
    "# trainer.train()\n",
    "\n",
    "print(\"Training code ready (uncomment to run with environment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Emergent Coordination\n",
    "\n",
    "After training, we can analyze emergent coordination patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ‚è±Ô∏è ~30-60 seconds\n\ndef analyze_coordination_patterns(policy, num_scenarios=10, num_aircraft=5):\n    \"\"\"\n    Analyze coordination patterns across multiple scenarios.\n    \"\"\"\n    all_attention = []\n    \n    for _ in range(num_scenarios):\n        # Generate random scenario\n        obs = {\n            \"aircraft\": torch.randn(1, 10, 14),\n            \"aircraft_mask\": torch.zeros(1, 10, dtype=torch.bool),\n            \"global_state\": torch.randn(1, 4)\n        }\n        obs[\"aircraft_mask\"][0, :num_aircraft] = True\n        \n        # Get communication\n        with torch.no_grad():\n            _, _, comm_attn = policy(obs, return_communication=True)\n        \n        # Store first layer attention\n        if comm_attn:\n            # Average over heads\n            attn = comm_attn[0][0].mean(dim=0).cpu().numpy()  # (num_agents, num_agents)\n            all_attention.append(attn[:num_aircraft, :num_aircraft])\n    \n    # Compute statistics\n    all_attention = np.array(all_attention)\n    mean_attn = all_attention.mean(axis=0)\n    std_attn = all_attention.std(axis=0)\n    \n    # Visualize\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Mean attention\n    sns.heatmap(\n        mean_attn,\n        annot=True,\n        fmt='.3f',\n        cmap='YlOrRd',\n        square=True,\n        ax=ax1\n    )\n    ax1.set_title(f'Mean Attention Across {num_scenarios} Scenarios')\n    ax1.set_xlabel('Key Agent')\n    ax1.set_ylabel('Query Agent')\n    \n    # Attention variance\n    sns.heatmap(\n        std_attn,\n        annot=True,\n        fmt='.3f',\n        cmap='Blues',\n        square=True,\n        ax=ax2\n    )\n    ax2.set_title('Attention Std Dev (Consistency)')\n    ax2.set_xlabel('Key Agent')\n    ax2.set_ylabel('Query Agent')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print insights\n    print(\"\\nCoordination Insights:\")\n    print(\"=\" * 60)\n    print(f\"Mean self-attention (diagonal): {np.diag(mean_attn).mean():.3f}\")\n    print(f\"Mean cross-attention (off-diagonal): {(mean_attn.sum() - np.diag(mean_attn).sum()) / (num_aircraft * (num_aircraft - 1)):.3f}\")\n    print(f\"\\nMost consistent attention (lowest variance): {std_attn.min():.3f}\")\n    print(f\"Most variable attention: {std_attn.max():.3f}\")\n\n# Analyze coordination (with untrained policy)\nanalyze_coordination_patterns(policy, num_scenarios=20, num_aircraft=5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Single-Agent vs Multi-Agent\n",
    "\n",
    "Compare the approaches conceptually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    \"Aspect\": [\n",
    "        \"Decision Making\",\n",
    "        \"Scalability\",\n",
    "        \"Communication\",\n",
    "        \"Training Complexity\",\n",
    "        \"Action Space\",\n",
    "        \"Coordination\",\n",
    "        \"Sample Efficiency\",\n",
    "        \"Emergent Behavior\"\n",
    "    ],\n",
    "    \"Single-Agent\": [\n",
    "        \"Centralized: selects one aircraft at a time\",\n",
    "        \"Difficult with many aircraft\",\n",
    "        \"Not modeled\",\n",
    "        \"Standard PPO\",\n",
    "        \"Discrete (select aircraft + command)\",\n",
    "        \"Learned implicitly\",\n",
    "        \"Good with stable environment\",\n",
    "        \"Limited\"\n",
    "    ],\n",
    "    \"Multi-Agent\": [\n",
    "        \"Decentralized: each aircraft decides independently\",\n",
    "        \"Naturally scales with aircraft count\",\n",
    "        \"Explicit via attention mechanism\",\n",
    "        \"MAPPO (more complex)\",\n",
    "        \"Per-agent actions (parallel)\",\n",
    "        \"Learned explicitly through communication\",\n",
    "        \"Better with cooperation\",\n",
    "        \"Rich coordination patterns\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nSingle-Agent vs Multi-Agent Comparison:\")\n",
    "print(\"=\" * 100)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Common Pitfalls & Troubleshooting\n\n### Problem 1: \"Non-stationary environment - training unstable\"\n**Solution**: This is inherent to multi-agent RL. Mitigate with:\n- **Centralized critic**: See global state to stabilize value estimates\n- **Parameter sharing**: Reduces variance across agents\n- **Larger replay buffer**: Smooths out non-stationarity\n\n```python\nconfig = MAPPOConfig(\n    buffer_size=100000,  # Increase buffer\n    batch_size=256,       # Larger batches\n)\n```\n\n### Problem 2: Agents ignore communication (all self-attention)\n**Causes**:\n- **Insufficient training**: Communication patterns emerge later\n- **Reward not cooperative**: Agents have no incentive to coordinate\n- **Attention not trained**: Check gradient flow to communication layers\n\n**Solution**: Add cooperation bonus to reward:\n```python\nreward = base_reward + 0.1 * coordination_bonus\n```\n\n### Problem 3: \"Variable agent count causes shape mismatches\"\n**Solution**: Always use aircraft_mask properly:\n```python\nobs = {\n    \"aircraft\": torch.randn(batch, max_aircraft, 14),\n    \"aircraft_mask\": mask,  # Critical! Mask inactive agents\n}\n```\n\n### Problem 4: Credit assignment problem - which agent to reward?\n**Inherent challenge in multi-agent RL**. MAPPO addresses with:\n- Value decomposition (each agent's contribution)\n- Global reward shared by all agents\n- Counterfactual baselines\n\n**Tip**: Monitor individual agent metrics to debug credit assignment.\n\n### Problem 5: Communication graph shows weak/uniform attention\n**Causes**:\n- **Model not trained**: Attention strengthens during training\n- **Too much regularization**: Reduce attention dropout\n- **Homogeneous scenarios**: Test with diverse conflicts\n\n### Problem 6: Scaling to many aircraft (20+) causes memory issues\n**Solution**: \n- Use gradient checkpointing\n- Reduce hidden dimensions\n- Process agents in chunks\n\n```python\nconfig = create_default_network_config(\n    hidden_dim=128,  # Reduce from 256\n    num_attention_heads=4,  # Reduce from 8\n)\n```\n\n### Debugging Tips:\n1. **Start with 3 agents**: Easier to visualize communication\n2. **Visualize attention every epoch**: Track evolution of coordination\n3. **Compare to single-agent**: Verify multi-agent actually helps\n4. **Check gradient norms**: Ensure all agents learning equally\n\n**Need more help?** See multi-agent RL surveys or MAPPO paper.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Key Insights\n\n### Advantages of Multi-Agent Formulation:\n\n1. **Natural Scalability**: Number of parameters doesn't grow with aircraft count (shared policy)\n2. **Parallel Decision Making**: All aircraft decide simultaneously (more realistic)\n3. **Explicit Communication**: Attention mechanism allows agents to share information\n4. **Emergent Coordination**: Cooperative behaviors emerge from communication\n5. **Decentralized Execution**: Robust to communication failures at test time\n\n### Challenges:\n\n1. **Training Complexity**: MAPPO is more complex than standard PPO\n2. **Non-Stationarity**: Each agent's environment changes as others learn\n3. **Credit Assignment**: Hard to determine which agent deserves credit/blame\n4. **Variable Agents**: Must handle aircraft spawning/exiting gracefully\n\n### Future Improvements:\n\n1. **Graph Neural Networks**: Use GNN for more sophisticated spatial reasoning\n2. **Hierarchical Communication**: Multi-level coordination (local clusters + global)\n3. **Curriculum Learning**: Start with few aircraft, gradually increase\n4. **Self-Play**: Agents learn against versions of themselves\n5. **Meta-Learning**: Learn to adapt to different traffic patterns"
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Inference Benchmark\n\nLet's benchmark the policy's inference speed to understand real-time performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ‚è±Ô∏è ~10-15 seconds for 1000 iterations\n\nimport time\n\ndef benchmark_inference(policy, num_iterations=1000, num_aircraft=5):\n    \"\"\"\n    Benchmark inference speed for the multi-agent policy.\n    \"\"\"\n    # Prepare observation\n    obs = {\n        \"aircraft\": torch.randn(1, 10, 14),\n        \"aircraft_mask\": torch.zeros(1, 10, dtype=torch.bool),\n        \"global_state\": torch.randn(1, 4)\n    }\n    obs[\"aircraft_mask\"][0, :num_aircraft] = True\n    \n    # Warm-up\n    with torch.no_grad():\n        for _ in range(10):\n            _ = policy(obs, return_communication=False)\n    \n    # Benchmark\n    start_time = time.time()\n    with torch.no_grad():\n        for _ in range(num_iterations):\n            _ = policy(obs, return_communication=False)\n    end_time = time.time()\n    \n    # Results\n    total_time = end_time - start_time\n    avg_time = total_time / num_iterations\n    fps = num_iterations / total_time\n    \n    print(f\"Inference Benchmark ({num_iterations} iterations, {num_aircraft} aircraft):\")\n    print(\"=\" * 60)\n    print(f\"Total time: {total_time:.3f} seconds\")\n    print(f\"Average time per inference: {avg_time*1000:.3f} ms\")\n    print(f\"Throughput: {fps:.1f} inferences/second\")\n    print(f\"\\nReal-time capability: {'‚úì Yes' if avg_time < 0.1 else '‚úó No'} (target: <100ms)\")\n\n# Run benchmark\nbenchmark_inference(policy, num_iterations=1000, num_aircraft=5)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}