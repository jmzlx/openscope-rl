{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple World Model for Fast Training\n",
        "\n",
        "This notebook demonstrates how to train a lightweight MLP-based dynamics model for OpenScope ATC, enabling fast model-based RL training without browser overhead.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "**Problem**: Browser-based training is slow (100-500ms per step), limiting RL iteration speed.\n",
        "\n",
        "**Solution**: Learn a simple dynamics model from collected trajectories, then train RL policies in the learned model (100x faster).\n",
        "\n",
        "**Simple World Model**:\n",
        "- MLP that predicts `next_state, reward = f(state, action)`\n",
        "- Trained on offline trajectories from OpenScope\n",
        "- Much simpler than Cosmos (no video, no foundation models)\n",
        "- Still enables fast model-based RL training\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Collect trajectories** from OpenScope (or use existing dataset)\n",
        "2. **Train simple dynamics model** - Learn state/action â†’ next_state/reward mapping\n",
        "3. **Evaluate model accuracy** - Test prediction quality\n",
        "4. **Train RL in learned model** - Fast training without browser\n",
        "5. **Compare performance** - Model-based vs browser-based training\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- OpenScope server running at http://localhost:3003 (for data collection)\n",
        "- GPU recommended for faster training\n",
        "- Estimated time: 30-45 minutes (data collection + training)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **World Models** - Learning environment dynamics to predict future states and rewards\n",
        "2. **Model-Based RL** - Training RL policies in learned models for faster iteration\n",
        "3. **Dynamics Model Architecture** - Simple MLP that predicts state transitions\n",
        "4. **Training Dynamics Models** - Supervised learning on trajectory data\n",
        "5. **Trade-offs** - Simplicity vs accuracy: when simple models are sufficient\n",
        "\n",
        "**Estimated Time**: 30-45 minutes (includes data collection and training)  \n",
        "**Prerequisites**: Understanding of supervised learning, basic RL concepts  \n",
        "**Hardware**: GPU recommended for faster training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Imports\n",
        "\n",
        "Let's set up the environment and import necessary modules.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup & Imports\n",
        "\n",
        "Set up imports and utilities for training the simple dynamics model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from data.offline_dataset import OfflineDatasetCollector\n",
        "from training.world_model_trainer import WorldModelTrainer, WorldModelConfig, DynamicsDataset, compute_state_dim, compute_action_dim\n",
        "from environment.utils import get_device\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Load Trajectories\n",
        "\n",
        "Load offline OpenScope trajectories (or collect a small demo set).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from data.offline_dataset import OfflineDatasetCollector\n",
        "\n",
        "# Load if available\n",
        "data_path = Path(\"../data/offline_data.pkl\")\n",
        "if data_path.exists():\n",
        "    print(f\"ðŸ“¦ Loading episodes from {data_path}\")\n",
        "    episodes = OfflineDatasetCollector.load_episodes(str(data_path))\n",
        "else:\n",
        "    print(\"âš ï¸ No dataset found at ../data/offline_data.pkl\")\n",
        "    episodes = []\n",
        "\n",
        "print(f\"Episodes: {len(episodes)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Build Dataset and Train World Model\n",
        "\n",
        "We'll flatten observations/actions and train the MLP dynamics model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(episodes) == 0:\n",
        "    raise RuntimeError(\"No offline episodes available. Please load or collect data first.\")\n",
        "\n",
        "# Compute flattened dims (use defaults; adjust if needed)\n",
        "state_dim = compute_state_dim(max_aircraft=20)\n",
        "action_dim = compute_action_dim()\n",
        "\n",
        "# Split episodes for train/val\n",
        "total_eps = len(episodes)\n",
        "val_eps = max(1, int(0.1 * total_eps))\n",
        "train_eps = total_eps - val_eps\n",
        "\n",
        "train_episodes = episodes[:train_eps]\n",
        "val_episodes = episodes[train_eps:]\n",
        "\n",
        "# Create datasets\n",
        "train_ds = DynamicsDataset(train_episodes, state_dim=state_dim, action_dim=action_dim)\n",
        "val_ds = DynamicsDataset(val_episodes, state_dim=state_dim, action_dim=action_dim)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# Configure and train\n",
        "config = WorldModelConfig(num_epochs=10, batch_size=64, learning_rate=1e-3)\n",
        "trainer = WorldModelTrainer(config, state_dim=state_dim, action_dim=action_dim)\n",
        "history = trainer.train(train_loader, val_loader)\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "print({k: v[-1] for k, v in history.items() if len(v) > 0})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply nest_asyncio for Jupyter compatibility\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from environment import PlaywrightEnv, create_default_config\n",
        "from data.offline_dataset import OfflineDatasetCollector\n",
        "from models.dynamics_model import (\n",
        "    SimpleDynamicsModel,\n",
        "    flatten_observation,\n",
        "    flatten_action,\n",
        "    compute_state_dim,\n",
        "    compute_action_dim,\n",
        ")\n",
        "from training.world_model_trainer import (\n",
        "    WorldModelTrainer,\n",
        "    WorldModelConfig,\n",
        "    DynamicsDataset,\n",
        ")\n",
        "from environment.utils import get_device\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Collect Trajectory Data\n",
        "\n",
        "First, we need to collect trajectories from OpenScope. We'll use a random policy to gather diverse state-action transitions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create OpenScope environment\n",
        "env = PlaywrightEnv(\n",
        "    airport=\"KLAS\",\n",
        "    max_aircraft=5,  # Start with small number for faster data collection\n",
        "    headless=True,\n",
        "    timewarp=5,\n",
        "    episode_length=600,  # 10 minutes\n",
        ")\n",
        "\n",
        "print(\"âœ… Environment created\")\n",
        "\n",
        "# Initialize collector\n",
        "collector = OfflineDatasetCollector(env)\n",
        "\n",
        "# Collect episodes (use fewer for demo - increase for real training)\n",
        "print(\"\\nðŸ“Š Collecting trajectory data...\")\n",
        "print(\"   This will take a few minutes. Using 50 episodes for demo.\")\n",
        "episodes = collector.collect_random_episodes(\n",
        "    num_episodes=50,  # Increase to 500-1000 for real training\n",
        "    max_steps=100,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Collected {len(episodes)} episodes\")\n",
        "print(f\"   Total timesteps: {sum(ep.length for ep in episodes)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Prepare Dataset for Dynamics Model Training\n",
        "\n",
        "We need to convert episodes into (state, action, next_state, reward) transitions for supervised learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute dimensions\n",
        "max_aircraft = 5\n",
        "state_dim = compute_state_dim(max_aircraft)\n",
        "action_dim = compute_action_dim()\n",
        "\n",
        "print(f\"State dimension (flattened): {state_dim}\")\n",
        "print(f\"Action dimension: {action_dim}\")\n",
        "\n",
        "# Create dataset\n",
        "dataset = DynamicsDataset(\n",
        "    episodes=episodes,\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        ")\n",
        "\n",
        "# Split into train/val\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Dataset prepared:\")\n",
        "print(f\"   Train samples: {len(train_dataset)}\")\n",
        "print(f\"   Val samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Train Dynamics Model\n",
        "\n",
        "Now we'll train the MLP to predict next states and rewards from current states and actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training\n",
        "config = WorldModelConfig(\n",
        "    hidden_dim=256,\n",
        "    num_layers=3,\n",
        "    dropout=0.1,\n",
        "    num_epochs=50,  # Reduce for demo\n",
        "    batch_size=64,\n",
        "    learning_rate=1e-3,\n",
        "    checkpoint_dir=\"../checkpoints/world_model\",\n",
        "    save_every=10,\n",
        "    device=get_device(),\n",
        ")\n",
        "\n",
        "print(\"ðŸ“‹ Training Configuration:\")\n",
        "print(f\"   Hidden dim: {config.hidden_dim}\")\n",
        "print(f\"   Layers: {config.num_layers}\")\n",
        "print(f\"   Epochs: {config.num_epochs}\")\n",
        "print(f\"   Batch size: {config.batch_size}\")\n",
        "print(f\"   Learning rate: {config.learning_rate}\")\n",
        "print(f\"   Device: {config.device}\")\n",
        "\n",
        "# Create trainer\n",
        "trainer = WorldModelTrainer(\n",
        "    config=config,\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Trainer created\")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nðŸš€ Starting training...\")\n",
        "history = trainer.train(train_loader, val_loader)\n",
        "\n",
        "print(\"\\nâœ… Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Evaluate Model Accuracy\n",
        "\n",
        "Let's evaluate how well the model predicts next states and rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "trainer.model.eval()\n",
        "\n",
        "state_errors = []\n",
        "reward_errors = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        states = batch[\"state\"].to(config.device)\n",
        "        actions = batch[\"action\"].to(config.device)\n",
        "        next_states_true = batch[\"next_state\"].to(config.device)\n",
        "        rewards_true = batch[\"reward\"].to(config.device).unsqueeze(1)\n",
        "        \n",
        "        # Predict\n",
        "        next_states_pred, rewards_pred = trainer.model(states, actions)\n",
        "        \n",
        "        # Compute errors\n",
        "        state_error = torch.mean((next_states_pred - next_states_true) ** 2, dim=1)\n",
        "        reward_error = torch.abs(rewards_pred - rewards_true).squeeze()\n",
        "        \n",
        "        state_errors.extend(state_error.cpu().numpy())\n",
        "        reward_errors.extend(reward_error.cpu().numpy())\n",
        "\n",
        "print(\"ðŸ“Š Model Evaluation Results:\")\n",
        "print(f\"   Mean state prediction MSE: {np.mean(state_errors):.4f}\")\n",
        "print(f\"   Mean reward prediction error: {np.mean(reward_errors):.4f}\")\n",
        "print(f\"   Reward error std: {np.std(reward_errors):.4f}\")\n",
        "\n",
        "# Visualize training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history[\"train_losses\"], label=\"Train\")\n",
        "if history.get(\"val_losses\"):\n",
        "    axes[0].plot(history[\"val_losses\"], label=\"Val\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"Training Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].hist(state_errors, bins=50, alpha=0.7)\n",
        "axes[1].set_xlabel(\"State Prediction MSE\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "axes[1].set_title(\"State Prediction Error Distribution\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Analysis and Comparison\n",
        "\n",
        "### Advantages of Simple World Model\n",
        "\n",
        "1. **Fast Training**: Can train RL policies 100x faster than browser-based training\n",
        "2. **Simple Architecture**: Just an MLP - no complex video models or foundation models  \n",
        "3. **Easy to Debug**: Simple architecture makes it easier to understand what's happening\n",
        "4. **Lower Resource Requirements**: No need for GPU clusters or large models\n",
        "\n",
        "### Limitations\n",
        "\n",
        "1. **Prediction Accuracy**: May not capture all dynamics accurately (especially complex interactions)\n",
        "2. **Distribution Shift**: Model trained on offline data may not generalize well to new scenarios\n",
        "3. **No Visual Information**: Only models state transitions, not visual cues\n",
        "\n",
        "### When to Use\n",
        "\n",
        "- âœ… You have enough offline data for accurate dynamics learning\n",
        "- âœ… Training speed is more important than perfect accuracy\n",
        "- âœ… You want to quickly iterate on RL algorithms\n",
        "- âœ… Resource constraints prevent using Cosmos\n",
        "\n",
        "### Comparison with Other Approaches\n",
        "\n",
        "- **vs Browser Training**: 100x faster, but potential accuracy loss\n",
        "- **vs Cosmos**: Much simpler and faster to train, but less accurate\n",
        "- **vs Decision Transformer**: Different approach - DT learns policies, world models learn dynamics\n",
        "\n",
        "## Summary\n",
        "\n",
        "We've successfully trained a simple MLP-based world model that can predict next states and rewards from current states and actions. This enables fast model-based RL training without the overhead of browser automation.\n",
        "\n",
        "**Next Steps:**\n",
        "- Train an RL policy in the learned model\n",
        "- Compare model-based vs browser-based training\n",
        "- Improve model accuracy with more data or better architecture\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
