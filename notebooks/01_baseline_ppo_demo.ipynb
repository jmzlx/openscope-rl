{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline PPO with Action Masking Demo\n",
    "\n",
    "This notebook demonstrates the baseline PPO approach for OpenScope RL with action masking.\n",
    "\n",
    "**Key Features:**\n",
    "- Action masking to prevent invalid actions\n",
    "- Small-scale training demo (10k steps)\n",
    "- Evaluation and visualization\n",
    "- Comparison vs random policy\n",
    "\n",
    "**Prerequisites:**\n",
    "- OpenScope server running at http://localhost:3003\n",
    "- All dependencies installed (`uv sync`)\n",
    "- Playwright browsers installed (`uv run playwright install chromium`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Action Masking** - How to prevent invalid actions and improve sample efficiency by 2-3x\n",
    "2. **PPO Training Loop** - The core mechanics of Proximal Policy Optimization\n",
    "3. **Metrics Tracking** - How to monitor training progress with separation violations, success rates, and rewards\n",
    "4. **Policy Evaluation** - Techniques for comparing trained agents against baselines\n",
    "5. **Model Persistence** - Saving and loading trained models for deployment\n",
    "\n",
    "**Estimated Time**: 15-20 minutes (10k training steps)  \n",
    "**Prerequisites**: OpenScope server running, dependencies installed  \n",
    "**Hardware**: CPU sufficient for demo (GPU recommended for full training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Environment Creation\n",
    "\n",
    "First, let's import dependencies and create an environment with action masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(sys.prefix)\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Disable tqdm notebook widgets to avoid JS rendering errors in some Jupyter frontends\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from environment import (\n",
    "    PlaywrightEnv,\n",
    "    create_action_mask_fn,\n",
    "    print_action_mask_summary,\n",
    "    ActionMaskingWrapper,\n",
    "    DictToMultiDiscreteWrapper,\n",
    ")\n",
    "from experiments.metrics import MetricsTracker\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment with Action Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base environment\n",
    "env = PlaywrightEnv(\n",
    "    airport=\"KLAS\",\n",
    "    max_aircraft=50,  # Increased cap for controlled aircraft\n",
    "    headless=False,\n",
    "    timewarp=1000,\n",
    "    episode_length=180,  # 2 minutes\n",
    ")\n",
    "\n",
    "# Convert Dict action space to MultiDiscrete for PPO compatibility\n",
    "env = DictToMultiDiscreteWrapper(env)\n",
    "\n",
    "# Apply action masking wrapper\n",
    "mask_fn = create_action_mask_fn(env)\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "print(f\"Environment created!\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Action Masking in Action\n",
    "\n",
    "Let's see how action masking works by examining a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Get aircraft data from the raw state\n",
    "aircraft_data = info[\"raw_state\"].get(\"aircraft\", [])\n",
    "\n",
    "# Get the base environment for accessing original action space\n",
    "base_env = env.unwrapped\n",
    "while hasattr(base_env, 'env'):\n",
    "    base_env = base_env.env\n",
    "    if not hasattr(base_env, 'env'):\n",
    "        break\n",
    "\n",
    "# Cap the demo to a small number of iterations to avoid long runs\n",
    "DEMO_MAX_STEPS = 5\n",
    "\n",
    "print(f\"\\nStep 0: {len(aircraft_data)} aircraft active\")\n",
    "print_action_mask_summary(\n",
    "    obs,\n",
    "    aircraft_data,\n",
    "    base_env.config.max_aircraft,\n",
    "    base_env.action_space,  # Original Dict action space\n",
    ")\n",
    "\n",
    "# Take a few random actions (capped)\n",
    "for i in range(DEMO_MAX_STEPS):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    aircraft_data = info[\"raw_state\"].get(\"aircraft\", [])\n",
    "    print(f\"Step {i+1}: {len(aircraft_data)} aircraft, reward={reward:.2f}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"\\nAction masking demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Small-Scale Training Demo\n",
    "\n",
    "Now let's train a PPO agent for 10k steps as a quick demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO model with MultiInputPolicy for Dict observation space\n",
    "# Note: ActionMasker wrapper works with MultiDiscrete action space (converted by DictToMultiDiscreteWrapper)\n",
    "model = PPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    ")\n",
    "\n",
    "print(\"✅ PPO model created!\")\n",
    "print(f\"   Policy: MultiInputPolicy\")\n",
    "print(f\"   Observation space: {env.observation_space}\")\n",
    "print(f\"   Action space: {env.action_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⏱️ Estimated runtime: 5-10 minutes (depends on your machine)\n",
    "# Progress bar will show ETA during training\n",
    "\n",
    "# Train for 10k steps\n",
    "model.learn(total_timesteps=10000, progress_bar=False)\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Evaluation with Visualizations\n",
    "\n",
    "Let's evaluate the trained agent and visualize its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(model, env, n_episodes=5):\n",
    "    \"\"\"Evaluate agent over multiple episodes.\"\"\"\n",
    "    tracker = MetricsTracker()\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        tracker.start_episode()\n",
    "        \n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from model\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Take step\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            tracker.update(reward, info)\n",
    "            \n",
    "            # Safety limit\n",
    "            if steps >= 200:\n",
    "                break\n",
    "        \n",
    "        # End episode tracking\n",
    "        metrics = tracker.end_episode(info.get(\"episode_metrics\", {}))\n",
    "        print(f\"Episode {episode+1}: reward={episode_reward:.2f}, steps={steps}\")\n",
    "    \n",
    "    return tracker\n",
    "\n",
    "print(\"Evaluating trained agent...\\n\")\n",
    "tracker = evaluate_agent(model, env, n_episodes=5)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n",
    "tracker.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress\n",
    "\n",
    "Let's plot some metrics to visualize the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode metrics\n",
    "episode_rewards = [m.total_reward for m in tracker.episode_metrics]\n",
    "episode_lengths = [m.episode_length for m in tracker.episode_metrics]\n",
    "success_rates = [m.success_rate for m in tracker.episode_metrics]\n",
    "violations = [m.separation_violations for m in tracker.episode_metrics]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(episode_rewards, marker='o')\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Episode lengths\n",
    "axes[0, 1].plot(episode_lengths, marker='o', color='orange')\n",
    "axes[0, 1].set_title('Episode Lengths')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Steps')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Success rates\n",
    "axes[1, 0].plot(success_rates, marker='o', color='green')\n",
    "axes[1, 0].set_title('Success Rates')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Success Rate')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Violations\n",
    "axes[1, 1].plot(violations, marker='o', color='red')\n",
    "axes[1, 1].set_title('Separation Violations')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Violations')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Comparison vs Random Policy\n",
    "\n",
    "Let's compare the trained agent against a random policy baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_policy(env, n_episodes=5, eval_max_steps=None, log_every=25):\n",
    "    \"\"\"Evaluate random policy.\"\"\"\n",
    "    tracker = MetricsTracker()\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        tracker.start_episode()\n",
    "        \n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Random action\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # Take step\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            tracker.update(reward, info)\n",
    "            \n",
    "            if log_every and steps % log_every == 0:\n",
    "                ac = info.get(\"aircraft_count\", len(info.get(\"raw_state\", {}).get(\"aircraft\", [])))\n",
    "                print(f\"  step={steps} r={reward:.2f} aircraft={ac} action={action}\")\n",
    "            \n",
    "            # Optional evaluation cap\n",
    "            if eval_max_steps is not None and steps >= eval_max_steps:\n",
    "                break\n",
    "        \n",
    "        metrics = tracker.end_episode(info.get(\"episode_metrics\", {}))\n",
    "        print(f\"Episode {episode+1}: reward={episode_reward:.2f}, steps={steps}\")\n",
    "    \n",
    "    return tracker\n",
    "\n",
    "print(\"Evaluating random policy...\\n\")\n",
    "random_tracker = evaluate_random_policy(env, n_episodes=5, eval_max_steps=None)\n",
    "\n",
    "print(\"\\nRandom policy evaluation complete!\")\n",
    "random_tracker.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summaries\n",
    "ppo_summary = tracker.get_summary()\n",
    "random_summary = random_tracker.get_summary()\n",
    "\n",
    "# Create comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: PPO vs Random Policy\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics_to_compare = [\n",
    "    (\"Avg Reward\", \"avg_total_reward\"),\n",
    "    (\"Avg Success Rate\", \"avg_success_rate\"),\n",
    "    (\"Avg Violations\", \"avg_separation_violations\"),\n",
    "    (\"Avg Episode Length\", \"avg_episode_length\"),\n",
    "]\n",
    "\n",
    "for name, key in metrics_to_compare:\n",
    "    ppo_val = ppo_summary.get(key, 0)\n",
    "    random_val = random_summary.get(key, 0)\n",
    "    \n",
    "    if \"rate\" in key.lower():\n",
    "        print(f\"{name:25s} | PPO: {ppo_val:.2%} | Random: {random_val:.2%}\")\n",
    "    else:\n",
    "        print(f\"{name:25s} | PPO: {ppo_val:.2f} | Random: {random_val:.2f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate improvement\n",
    "reward_improvement = ((ppo_summary.get(\"avg_total_reward\", 0) - \n",
    "                       random_summary.get(\"avg_total_reward\", 0)) / \n",
    "                      abs(random_summary.get(\"avg_total_reward\", 1)) * 100)\n",
    "\n",
    "print(f\"\\nReward Improvement: {reward_improvement:+.1f}%\")\n",
    "print(\"\\nNote: This is a small demo with only 10k training steps.\")\n",
    "print(\"For full training (500k+ steps), expect much better performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Save Model and Results\n",
    "\n",
    "Let's save the trained model and results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper function to convert numpy types to native Python types for JSON serialization\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Recursively convert numpy types to native Python types.\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"../results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save model\n",
    "model_path = results_dir / f\"ppo_demo_{timestamp}.zip\"\n",
    "model.save(str(model_path))\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"training_steps\": 10000,\n",
    "    \"ppo_summary\": ppo_summary,\n",
    "    \"random_summary\": random_summary,\n",
    "    \"config\": {\n",
    "        \"max_aircraft\": 5,\n",
    "        \"airport\": \"KLAS\",\n",
    "        \"episode_length\": 600,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Convert numpy types to native Python types before JSON serialization\n",
    "results = convert_to_serializable(results)\n",
    "\n",
    "results_path = results_dir / f\"results_{timestamp}.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "print(\"\\nDemo complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override evaluation helper with extended controls and logging\n",
    "\n",
    "def evaluate_agent(model, env, n_episodes=5, eval_max_steps=None, log_every=25):\n",
    "    \"\"\"Evaluate agent over multiple episodes with optional horizon and logging.\"\"\"\n",
    "    tracker = MetricsTracker()\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        tracker.start_episode()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            tracker.update(reward, info)\n",
    "            if log_every and steps % log_every == 0:\n",
    "                ac = info.get(\"aircraft_count\", len(info.get(\"raw_state\", {}).get(\"aircraft\", [])))\n",
    "                print(f\"  step={steps} r={reward:.2f} aircraft={ac} action={action}\")\n",
    "            if eval_max_steps is not None and steps >= eval_max_steps:\n",
    "                break\n",
    "        tracker.end_episode(info.get(\"episode_metrics\", {}))\n",
    "        print(f\"Episode {episode+1}: reward={episode_reward:.2f}, steps={steps}\")\n",
    "    return tracker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Don't forget to close the environment to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "print(\"Environment closed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Common Pitfalls & Troubleshooting\n",
    "\n",
    "### Problem 1: \"OpenScope server not responding\" or \"Navigation failed with HTTP 503\"\n",
    "**Solution**: Ensure the OpenScope server is running at http://localhost:3003\n",
    "```bash\n",
    "cd ../../../openscope\n",
    "npm start\n",
    "```\n",
    "\n",
    "### Problem 2: Training crashes with \"Browser process exited unexpectedly\"\n",
    "**Solution**: Playwright browsers may not be installed or headless mode is causing issues\n",
    "```bash\n",
    "uv run playwright install chromium\n",
    "# Or try headless=False for debugging\n",
    "```\n",
    "\n",
    "### Problem 3: Training is very slow (>30 minutes for 10k steps)\n",
    "**Causes & Solutions**:\n",
    "- **Timewarp too low**: Increase to `timewarp=10` or higher\n",
    "- **Too many aircraft**: Reduce `max_aircraft=3` for faster episodes\n",
    "- **Headless=False**: Set `headless=True` to disable rendering\n",
    "\n",
    "### Problem 4: Poor performance after training (random-level rewards)\n",
    "**Causes**:\n",
    "- **Insufficient training**: 10k steps is just a demo - use 500k+ for real performance\n",
    "- **Action masking not working**: Check that `ActionMasker` wrapper is applied\n",
    "- **Episode too short**: Increase `episode_length` to give agent more learning opportunities\n",
    "\n",
    "### Problem 5: \"Action space mismatch\" or shape errors\n",
    "**Solution**: Ensure environment is wrapped with ActionMasker AFTER creation:\n",
    "```python\n",
    "env = PlaywrightEnv(...)  # Create first\n",
    "env = ActionMasker(env, mask_fn)  # Then wrap\n",
    "```\n",
    "\n",
    "### Problem 6: Notebook cells run out of order\n",
    "**Solution**: Always run cells sequentially from top to bottom. Use \"Restart Kernel & Run All\" if unsure.\n",
    "\n",
    "### Debugging Tips:\n",
    "1. **Check logs**: Look for error messages in the output\n",
    "2. **Reduce complexity**: Start with `max_aircraft=2` to isolate issues\n",
    "3. **Test environment**: Run cell-4 alone to verify environment creation works\n",
    "4. **Monitor GPU**: Use `nvidia-smi` to check GPU utilization (if using GPU)\n",
    "\n",
    "**Need more help?** Check the main README or open an issue on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This demo showed basic PPO training with action masking over 10k steps. For production training:\n",
    "\n",
    "1. **Scale up training**: Use `training/ppo_trainer.py` for 500k+ steps\n",
    "2. **Use parallel environments**: 8+ environments for faster training\n",
    "3. **Enable curriculum learning**: Gradually increase difficulty\n",
    "4. **Track with WandB**: Monitor training progress in real-time\n",
    "5. **Evaluate on benchmarks**: Use `experiments/benchmark.py` for standardized evaluation\n",
    "\n",
    "See the main README for full training instructions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
