{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TD-MPC 2: Model-Based RL with Transformer World Model\n",
        "\n",
        "This notebook demonstrates TD-MPC 2 (Temporal Difference Model Predictive Control 2) for OpenScope ATC, combining:\n",
        "- **Transformer-based world model** for dynamics prediction\n",
        "- **Model Predictive Control (MPC)** for action planning\n",
        "- **Q-learning** for long-term value estimation\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "**TD-MPC 2** is a sample-efficient model-based RL algorithm that:\n",
        "- Learns a transformer-based world model in latent space\n",
        "- Uses MPC with Cross-Entropy Method (CEM) for action planning\n",
        "- Combines short-term planning (MPC) with long-term value (Q-function)\n",
        "- Achieves better sample efficiency than pure model-free methods\n",
        "\n",
        "**Workflow**:\n",
        "1. **Collect initial data** from OpenScope (random or heuristic policy)\n",
        "2. **Train world model** - Learn dynamics and reward prediction\n",
        "3. **Train Q-function** - Learn long-term value estimates\n",
        "4. **MPC planning** - Use learned model for action selection\n",
        "5. **Online learning** - Collect data with MPC policy and continue training\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- OpenScope server running at http://localhost:3003\n",
        "- GPU recommended for faster training\n",
        "- Estimated time: 2-3 hours for full training (1M steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **TD-MPC 2 Algorithm** - How transformer world models combine with MPC and Q-learning\n",
        "2. **Latent State Representation** - Encoding observations to compact latent space\n",
        "3. **MPC Planning** - Cross-Entropy Method for action optimization\n",
        "4. **Joint Training** - Simultaneously learning dynamics, rewards, and Q-values\n",
        "5. **Sample Efficiency** - How model-based RL reduces environment interactions\n",
        "\n",
        "**Estimated Time**: 2-3 hours for full training (1M steps) | 30 min for quick demo (10k steps)  \n",
        "**Prerequisites**: Understanding of transformers, Q-learning, model-based RL  \n",
        "**Hardware**: GPU strongly recommended\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup & Imports\n",
        "\n",
        "Set up imports and create the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from environment import PlaywrightEnv\n",
        "from models.tdmpc2 import TDMPC2Model, TDMPC2Config\n",
        "from training.tdmpc2_trainer import TDMPC2Trainer, TDMPC2TrainingConfig, ReplayBuffer\n",
        "from training.tdmpc2_planner import MPCPlanner, MPCPlannerConfig\n",
        "from environment.utils import get_device\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Create Environment\n",
        "\n",
        "Create the OpenScope environment for data collection and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment configuration\n",
        "AIRPORT = \"KLAS\"\n",
        "MAX_AIRCRAFT = 10\n",
        "HEADLESS = True  # Set to False to see browser\n",
        "TIMEWARP = 5\n",
        "\n",
        "# Create environment\n",
        "env = PlaywrightEnv(\n",
        "    airport=AIRPORT,\n",
        "    max_aircraft=MAX_AIRCRAFT,\n",
        "    headless=HEADLESS,\n",
        "    timewarp=TIMEWARP,\n",
        ")\n",
        "\n",
        "print(f\"âœ… Environment created: {AIRPORT} with max {MAX_AIRCRAFT} aircraft\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Configure TD-MPC 2\n",
        "\n",
        "Set up model and training configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "model_config = TDMPC2Config(\n",
        "    aircraft_feature_dim=14,\n",
        "    global_feature_dim=4,\n",
        "    max_aircraft=MAX_AIRCRAFT,\n",
        "    latent_dim=512,\n",
        "    encoder_hidden_dim=256,\n",
        "    encoder_num_layers=4,\n",
        "    encoder_num_heads=8,\n",
        "    dynamics_hidden_dim=512,\n",
        "    dynamics_num_layers=3,\n",
        "    action_dim=5,  # aircraft_id, command_type, altitude, heading, speed\n",
        ")\n",
        "\n",
        "# Planner configuration\n",
        "planner_config = MPCPlannerConfig(\n",
        "    planning_horizon=5,\n",
        "    num_samples=512,\n",
        "    num_elites=64,\n",
        "    num_iterations=6,\n",
        "    gamma=0.99,\n",
        ")\n",
        "\n",
        "# Training configuration\n",
        "training_config = TDMPC2TrainingConfig(\n",
        "    model_config=model_config,\n",
        "    planner_config=planner_config,\n",
        "    num_steps=100000,  # Reduced for demo - use 1000000 for full training\n",
        "    batch_size=64,\n",
        "    learning_rate_model=1e-3,\n",
        "    learning_rate_q=1e-3,\n",
        "    buffer_capacity=100000,\n",
        "    min_buffer_size=1000,\n",
        "    eval_frequency=5000,\n",
        "    eval_episodes=5,\n",
        "    checkpoint_dir=\"checkpoints/tdmpc2\",\n",
        "    use_wandb=False,  # Set to True to enable WandB logging\n",
        ")\n",
        "\n",
        "print(\"âœ… Configuration created\")\n",
        "print(f\"Model latent dim: {model_config.latent_dim}\")\n",
        "print(f\"Planning horizon: {planner_config.planning_horizon}\")\n",
        "print(f\"Training steps: {training_config.num_steps}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Create Model and Trainer\n",
        "\n",
        "Initialize the TD-MPC 2 model and trainer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = TDMPC2Model(model_config)\n",
        "print(f\"âœ… Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "# Create trainer\n",
        "trainer = TDMPC2Trainer(env, training_config)\n",
        "print(\"âœ… Trainer initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Test MPC Planning\n",
        "\n",
        "Test the MPC planner on a single observation to verify it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset environment and get observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Convert to tensors\n",
        "aircraft_tensor = torch.from_numpy(obs[\"aircraft\"]).float().unsqueeze(0).to(model_config.device)\n",
        "mask_tensor = torch.from_numpy(obs[\"aircraft_mask\"]).bool().unsqueeze(0).to(model_config.device)\n",
        "global_tensor = torch.from_numpy(obs[\"global_state\"]).float().unsqueeze(0).to(model_config.device)\n",
        "\n",
        "# Test planner\n",
        "planner = MPCPlanner(model, planner_config)\n",
        "with torch.no_grad():\n",
        "    action = planner.plan(aircraft_tensor, mask_tensor, global_tensor)\n",
        "\n",
        "print(f\"âœ… MPC planning successful\")\n",
        "print(f\"Planned action shape: {action.shape}\")\n",
        "print(f\"Action values: {action.squeeze().cpu().numpy()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Train TD-MPC 2\n",
        "\n",
        "Start training! This will:\n",
        "1. Collect initial data to fill replay buffer\n",
        "2. Train world model and Q-function jointly\n",
        "3. Use MPC policy for data collection\n",
        "4. Continue online learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"ðŸš€ Starting TD-MPC 2 training...\")\n",
        "print(\"This will take a while. Progress will be shown below.\")\n",
        "print()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Evaluate Trained Model\n",
        "\n",
        "Evaluate the trained model on the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "eval_metrics = trainer._evaluate()\n",
        "\n",
        "print(\"ðŸ“Š Evaluation Results:\")\n",
        "for key, value in eval_metrics.items():\n",
        "    print(f\"  {key}: {value:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Visualize Training Progress\n",
        "\n",
        "Plot training metrics if available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If WandB was used, metrics are logged there\n",
        "# Otherwise, you can track metrics manually during training\n",
        "\n",
        "print(\"ðŸ“ˆ Training metrics:\")\n",
        "print(f\"  Training steps: {trainer.training_step}\")\n",
        "print(f\"  Environment steps: {trainer.env_step}\")\n",
        "print(f\"  Total episodes: {trainer.episode}\")\n",
        "print(f\"  Replay buffer size: {len(trainer.replay_buffer)}\")\n",
        "print(f\"  Best eval return: {trainer.best_eval_return:.2f}\")\n",
        "\n",
        "# Note: For detailed plots, enable WandB logging in training_config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Save and Load Checkpoints\n",
        "\n",
        "Save the trained model and demonstrate loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final checkpoint\n",
        "trainer.save_checkpoint(\"final_tdmpc2_model.pt\")\n",
        "print(\"âœ… Model saved\")\n",
        "\n",
        "# Example: Load checkpoint\n",
        "# trainer.load_checkpoint(\"checkpoints/tdmpc2/final_model.pt\")\n",
        "# print(\"âœ… Model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **TD-MPC 2 Architecture** - Transformer-based world model with MPC planning\n",
        "2. **Model Training** - Joint learning of dynamics, rewards, and Q-values\n",
        "3. **MPC Planning** - Cross-Entropy Method for action optimization\n",
        "4. **Online Learning** - Continuous improvement through environment interaction\n",
        "\n",
        "**Next Steps**:\n",
        "- Experiment with different model architectures (latent_dim, num_layers)\n",
        "- Tune MPC parameters (planning_horizon, num_samples)\n",
        "- Compare with other model-based methods (DreamerV3, Trajectory Transformer)\n",
        "- Enable WandB logging for detailed metrics visualization\n",
        "\n",
        "**References**:\n",
        "- TD-MPC 2 Paper: https://arxiv.org/abs/2310.16828\n",
        "- Original TD-MPC: https://arxiv.org/abs/2203.04955\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
